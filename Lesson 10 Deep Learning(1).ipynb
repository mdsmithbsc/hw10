{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lesson 10 – Deep Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_This notebook contains all the sample code and solutions to the exercises in chapter 11._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's make sure this notebook works well in both python 2 and 3, import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To support both python 2 and python 3\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "def reset_graph(seed=42):\n",
    "    tf.reset_default_graph()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "# To plot pretty figures\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "CHAPTER_ID = \"deep\"\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True):\n",
    "    path = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID, fig_id + \".png\")\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format='png', dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep neural networks have more than one hidden layes. As we add more layers, we may face several problems:\n",
    "    1. Vanishing gradients problem (or the related exploding gradients problem) that affects deep neural networks and makes lower layers very hard to train.\n",
    "    2. Slow Training\n",
    "    3. Risk overfitting the training set\n",
    "  We will look at these problem and try to solve them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanishing/Exploding Gradients Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backpropagation algorithm works by going from the output layer to the input layer, propagating the error gradient on the way. Once the algorithm has computed the gradient of the cost function with regards to each parameter in the network, it uses these gradients to update each parameter with a Gradient Descent step. <br>\n",
    "\n",
    "Unfortunately, gradients often get smaller and smaller as the algorithm progresses down to the lower layers. As a result, the Gradient Descent update leaves the lower layer connection weights virtually unchanged, and training never converges to a good solution. This is called the vanishing gradients problem. <br>\n",
    "\n",
    "In some cases, the opposite can happen: the gradients can grow bigger and bigger, so many layers get insanely large weight updates and the algorithm diverges. This is the exploding gradients problem, which is mostly encountered in recurrent neural networks. More generally, deep neural networks suffer from unstable gradients; different layers may learn at widely different speeds. <br>\n",
    "\n",
    "This problem was largely unresolved until 2010 A paper titled “Understanding the Difficulty of Training\n",
    "Deep Feedforward Neural Networks” by Xavier Glorot and Yoshua Bengi found that random weight initialization using a normal distribution with a mean of 0 and a standard deviation of 1 fixes the problem. If this is not done, the variance is increases with each additional layer. <br>\n",
    "\n",
    "This is actually made worse by the fact that the logistic function has a mean of 0.5, not 0 (the hyperbolic tangent function has a mean of 0 and behaves slightly better than the logistic function in deep networks). <br>\n",
    "\n",
    "In Logistic activation function inputs may become large (negative or positive) and the function saturates at 0 or 1, with a derivative extremely close to 0. When backpropagation starts, it has little gradient to propagate back through the network, and this small gradient keeps getting diluted as backpropagation progresses down through the top layers, so there is really nothing left for the lower layers. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure sigmoid_saturation_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "z = np.linspace(-5, 5, 200)\n",
    "\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [1, 1], 'k--')\n",
    "plt.plot([0, 0], [-0.2, 1.2], 'k-')\n",
    "plt.plot([-5, 5], [-3/4, 7/4], 'g--')\n",
    "plt.plot(z, logit(z), \"b-\", linewidth=2)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Saturating', xytext=(3.5, 0.7), xy=(5, 1), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Saturating', xytext=(-3.5, 0.3), xy=(-5, 0), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.annotate('Linear', xytext=(2, 0.2), xy=(0, 0.5), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.grid(True)\n",
    "plt.title(\"Sigmoid activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.2, 1.2])\n",
    "\n",
    "save_fig(\"sigmoid_saturation_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xavier and He Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The signal need to signal to flow properly in both directions: in the forward direction when making predictions, and in the\n",
    "reverse direction when backpropagating gradients without dying or saturation. <br>\n",
    "\n",
    "The proposed solution by Glorot and Bengio requires equal variance in inputs and outputs. It is actually not possible unless the layer has an equal number of input and output connections. But there is a good compromise: <br>\n",
    "\n",
    "The connection weights must be initialized randomly according to the following formula (Xavier or Glorot initialization) of logistic activation function. <br>\n",
    "\n",
    "Normal distribution with mean 0 and standard deviation $\\sigma=\\sqrt{\\frac{2}{n_{inputs} + n_{outputs}}}$ <br>\n",
    "\n",
    "For uniform distribution $U(-r,r)$ with $r=\\sqrt{\\frac{6}{n_{inputs} + n_{outputs}}}$ <br>\n",
    "\n",
    "When the number of of connections is equal it can be simplified to $\\sigma = \\sqrt{\\frac{1}{n_{inputs}}}$\n",
    "\n",
    "For ReLU\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use *he_init* for He initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load He initializer\n",
    "he_init = tf.variance_scaling_initializer()\n",
    "# Use it to specify a hidden neural level.\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                          kernel_initializer=he_init, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonsaturating Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glorot and Bengio show that the vanishing/exploding gradients problems were in part due to a poor choice of activation function. Sigmoid function copied from biology is not a good choice. ReLU activation function performs much better, mostly it does not produce extreme values and is fast to compute. <br>\n",
    "\n",
    "\n",
    "ReLU wiht large learning rate suffers from a problem known as the dying neurons when they stop outputting anything other than 0. If the updated weights produce $z<0$ the ReLU function will return 0 and the gradiend will be also 0. This neuron is likely to stay dead. <br>\n",
    "\n",
    "There a few ways to solve this problem: <br>\n",
    "\n",
    "1. Leaky ReLU defined as $LeakyReLU \\alpha(z) = \\max(\\alpha z, z)$. The hyperparameter $\\alpha$ defines how much the function “leaks”: it is the slope of the function for z < 0, and is typically set to 0.01.  This small slope ensures that leaky ReLUs never die; they can go into a long coma, but they have a chance to eventually wake up. Leaky ReLU tends to outperfrom ReLU activation function. Large $\\alpha=0.2$ sometimes performs better than 0.01. \n",
    "\n",
    "2. Randomized Leaky ReLU (RReLU), where $\\alpha$ is picked randomly during training and fixed during testing. This randomization reduces overfitting.\n",
    "\n",
    "3. Parametric Leaky ReLU (PReLU) where we optimeze parameter $\\alpha$ during backpropagation. This methods overperforms other ReLU on large datasets, but tends to overfit smaller ones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leaky ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.maximum(alpha*z, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure leaky_relu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, leaky_relu(z, 0.05), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([0, 0], [-0.5, 4.2], 'k-')\n",
    "plt.grid(True)\n",
    "props = dict(facecolor='black', shrink=0.1)\n",
    "plt.annotate('Leak', xytext=(-3.5, 0.5), xy=(-5, -0.2), arrowprops=props, fontsize=14, ha=\"center\")\n",
    "plt.title(\"Leaky ReLU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -0.5, 4.2])\n",
    "\n",
    "save_fig(\"leaky_relu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing Leaky ReLU in TensorFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is not prepackaged LReLU function, but we can define it and program a network layer with it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(z, name=None):\n",
    "    return tf.maximum(0.01 * z, z, name=name)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a neural network on MNIST using the Leaky ReLU. First let's create the graph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify two hidden layers with 300, and 100 neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=leaky_relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=leaky_relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify loss function for error calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set learning rate at 0.01. Use GradientDescent to minimize loss function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set-up predictions of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**: `tf.examples.tutorials.mnist` is deprecated. We will use `tf.keras.datasets.mnist` instead. <br>\n",
    "Load first 5000 obs into validation and other into training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "X_train = X_train.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "X_test = X_test.astype(np.float32).reshape(-1, 28*28) / 255.0\n",
    "y_train = y_train.astype(np.int32)\n",
    "y_test = y_test.astype(np.int32)\n",
    "X_valid, X_train = X_train[:5000], X_train[5000:]\n",
    "y_valid, y_train = y_train[:5000], y_train[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup batch processing of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_batch(X, y, batch_size):\n",
    "    rnd_idx = np.random.permutation(len(X))\n",
    "    n_batches = len(X) // batch_size\n",
    "    for batch_idx in np.array_split(rnd_idx, n_batches):\n",
    "        X_batch, y_batch = X[batch_idx], y[batch_idx]\n",
    "        yield X_batch, y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.86 Validation accuracy: 0.9044\n",
      "5 Batch accuracy: 0.94 Validation accuracy: 0.9494\n",
      "10 Batch accuracy: 0.92 Validation accuracy: 0.9656\n",
      "15 Batch accuracy: 0.94 Validation accuracy: 0.971\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9762\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.9772\n",
      "30 Batch accuracy: 0.98 Validation accuracy: 0.9782\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9788\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Djork-Arné Clevert et al. the exponential linear unit (ELU) activation function, it tends to outperform other ReLU both in time and testing accuracy:\n",
    "$$ELU_{\\alpha}(z) = \\begin{cases} \\alpha(\\exp(z) -1) \\quad &if \\quad z<0 \\\\ z \\quad &if \\quad z \\geq 0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks a lot like the ReLU function, with a few major differences:\n",
    "1. It take negative values when z < 0, which allows the unit to have an average output closer to 0 helping to alleviate vanishing gradients problem.\n",
    "2. The hyperparameter $\\alpha$ defines the value that the ELU function approaches when z is a large negative number. It is usually set to 1, but you can be tweaked.\n",
    "3. ELU has a nonzero gradient for $z < 0$, which avoids the dying units issue.\n",
    "4. ELU is is smooth everywhere, including around z = 0, which helps speed up Gradient Descent. <br>\n",
    "\n",
    "\n",
    "The main drawback of the ELU activation function is that it is slower to compute than the ReLU and its\n",
    "variants (due to the use of the exponential function), but during training this is compensated by the faster\n",
    "convergence rate. However, at test time an ELU network will be slower than a ReLU network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elu(z, alpha=1):\n",
    "    return np.where(z < 0, alpha * (np.exp(z) - 1), z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure elu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, elu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1, -1], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"ELU activation function ($\\alpha=1$)\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"elu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing ELU in TensorFlow is trivial, just specify the activation function when building each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow has a pre-defined ELU function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.elu, name=\"hidden1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This activation function was proposed in this [great paper](https://arxiv.org/pdf/1706.02515.pdf) by Günter Klambauer, Thomas Unterthiner and Andreas Mayr, 2017. During training, a neural network composed of a stack of dense layers using the SELU activation function will self-normalize: <br>\n",
    "\n",
    "The output of each layer will tend to preserve the same mean and variance during training, which solves the vanishing/exploding gradients problem. As a result, this activation function outperforms the other activation functions very significantly for such neural nets, so you should really try it out.\n",
    "$$selu(x)=\\lambda \\begin{cases} \\alpha\\exp(z) - \\alpha \\quad &if \\quad z<0 \\\\ x \\quad &if \\quad z \\geq 0 \\end{cases}$$\n",
    "We solve for $\\alpha$ and $\\lambda$ to find a fixed point that preserves mean and variance from layer to layer:\n",
    "$$g(\\mu, v, \\alpha, \\lambda) = (\\mu, v)$$\n",
    "Gradients neigher explode nor vanish. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * elu(z, alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving figure selu_plot\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(z, selu(z), \"b-\", linewidth=2)\n",
    "plt.plot([-5, 5], [0, 0], 'k-')\n",
    "plt.plot([-5, 5], [-1.758, -1.758], 'k--')\n",
    "plt.plot([0, 0], [-2.2, 3.2], 'k-')\n",
    "plt.grid(True)\n",
    "plt.title(r\"SELU activation function\", fontsize=14)\n",
    "plt.axis([-5, 5, -2.2, 3.2])\n",
    "\n",
    "save_fig(\"selu_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, the SELU hyperparameters (`scale` and `alpha`) are tuned in such a way that the mean remains close to 0, and the standard deviation remains close to 1 (assuming the inputs are standardized with mean 0 and standard deviation 1 too). Using this activation function, even a 100 layer deep neural network preserves roughly mean 0 and standard deviation 1 across all layers, avoiding the exploding/vanishing gradients problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0: -0.26 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 10: -0.24 < mean < 0.27, 0.74 < std deviation < 1.27\n",
      "Layer 20: -0.17 < mean < 0.18, 0.74 < std deviation < 1.24\n",
      "Layer 30: -0.27 < mean < 0.24, 0.78 < std deviation < 1.20\n",
      "Layer 40: -0.38 < mean < 0.39, 0.74 < std deviation < 1.25\n",
      "Layer 50: -0.27 < mean < 0.31, 0.73 < std deviation < 1.27\n",
      "Layer 60: -0.26 < mean < 0.43, 0.74 < std deviation < 1.35\n",
      "Layer 70: -0.19 < mean < 0.21, 0.75 < std deviation < 1.21\n",
      "Layer 80: -0.18 < mean < 0.16, 0.72 < std deviation < 1.19\n",
      "Layer 90: -0.19 < mean < 0.16, 0.75 < std deviation < 1.20\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "Z = np.random.normal(size=(500, 100))\n",
    "for layer in range(100):\n",
    "    W = np.random.normal(size=(100, 100), scale=np.sqrt(1/100))\n",
    "    Z = selu(np.dot(Z, W))\n",
    "    means = np.mean(Z, axis=1)\n",
    "    stds = np.std(Z, axis=1)\n",
    "    if layer % 10 == 0:\n",
    "        print(\"Layer {}: {:.2f} < mean < {:.2f}, {:.2f} < std deviation < {:.2f}\".format(\n",
    "            layer, means.min(), means.max(), stds.min(), stds.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.nn.selu()` function was added in TensorFlow 1.4. For earlier versions, you can use the following implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selu(z,\n",
    "         scale=1.0507009873554804934193349852946,\n",
    "         alpha=1.6732632423543772848170429916717):\n",
    "    return scale * tf.where(z >= 0.0, z, alpha * tf.nn.elu(z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the SELU activation function cannot be used along with regular Dropout (this would cancel the SELU activation function's self-normalizing property). Fortunately, there is a Dropout variant called Alpha Dropout proposed in the same paper. It is available in `tf.contrib.nn.alpha_dropout()` since TF 1.4 (or check out [this implementation](https://github.com/bioinf-jku/SNNs/blob/master/selu.py) by the Institute of Bioinformatics, Johannes Kepler University Linz)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a neural net for MNIST using the SELU activation function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# Set-up two hiddne layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train it. Do not forget to scale the inputs to mean 0 and standard deviation 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.88 Validation accuracy: 0.9232\n",
      "5 Batch accuracy: 0.98 Validation accuracy: 0.9574\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9662\n",
      "15 Batch accuracy: 0.96 Validation accuracy: 0.9684\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9692\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.969\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9694\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9702\n"
     ]
    }
   ],
   "source": [
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training data accuracy quickly approached 1, we probably overfitting. The main advantage of SELU is in deeper networks. It takes long time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Batch accuracy: 0.98 Validation accuracy: 0.946\n",
      "5 Batch accuracy: 1.0 Validation accuracy: 0.9682\n",
      "10 Batch accuracy: 1.0 Validation accuracy: 0.9722\n",
      "15 Batch accuracy: 0.98 Validation accuracy: 0.972\n",
      "20 Batch accuracy: 1.0 Validation accuracy: 0.9724\n",
      "25 Batch accuracy: 1.0 Validation accuracy: 0.972\n",
      "30 Batch accuracy: 1.0 Validation accuracy: 0.9728\n",
      "35 Batch accuracy: 1.0 Validation accuracy: 0.9734\n"
     ]
    }
   ],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 2000\n",
    "n_hidden2 = 500\n",
    "n_hidden3 = 200\n",
    "n_hidden4 = 60\n",
    "n_hidden5 = 30\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "# Set-up two hiddne layers\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=selu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=selu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=selu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=selu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=selu, name=\"hidden5\")\n",
    "\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "n_epochs = 40\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "means = X_train.mean(axis=0, keepdims=True)\n",
    "stds = X_train.std(axis=0, keepdims=True) + 1e-10\n",
    "X_val_scaled = (X_valid - means) / stds\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            X_batch_scaled = (X_batch - means) / stds\n",
    "            sess.run(training_op, feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "        if epoch % 5 == 0:\n",
    "            acc_batch = accuracy.eval(feed_dict={X: X_batch_scaled, y: y_batch})\n",
    "            acc_valid = accuracy.eval(feed_dict={X: X_val_scaled, y: y_valid})\n",
    "            print(epoch, \"Batch accuracy:\", acc_batch, \"Validation accuracy:\", acc_valid)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final_selu.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tensorflow.contrib.layers.batch_norm()` rather than `tf.layers.batch_normalization()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.batch_normalization()`, because anything in the contrib module may change or be deleted without notice. Instead of using the `batch_norm()` function as a regularizer parameter to the `fully_connected()` function, we now use `batch_normalization()` and we explicitly create a distinct layer. The parameters are a bit different, in particular:\n",
    "* `decay` is renamed to `momentum`,\n",
    "* `is_training` is renamed to `training`,\n",
    "* `updates_collections` is removed: the update operations needed by batch normalization are added to the `UPDATE_OPS` collection and you need to explicity run these operations during training (see the execution phase below),\n",
    "* we don't need to specify `scale=True`, as that is the default.\n",
    "\n",
    "Also note that in order to run batch norm just _before_ each hidden layer's activation function, we apply the ELU activation function manually, right after the batch norm layer.\n",
    "\n",
    "Note: since the `tf.layers.dense()` function is incompatible with `tf.contrib.layers.arg_scope()` (which is used in the book), we now use python's `functools.partial()` function instead. It makes it easy to create a `my_dense_layer()` function that just calls `tf.layers.dense()` with the desired parameters automatically set (unless they are overridden when calling `my_dense_layer()`). As you can see, the code remains very similar.\n",
    "\n",
    "# I skip this material because it dupclicate SELU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Clipping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A popular technique to lessen the exploding gradients problem is to simply clip the gradients during\n",
    "backpropagation so that they never exceed some threshold (this is mostly useful for recurrent neural\n",
    "networks; see Chapter 14). <br>\n",
    "\n",
    "In general people now prefer Batch Normalization/SELU, but it’s still useful to know about Gradient Clipping and how to implement it.In TensorFlow, the optimizer’s minimize() function takes care of both computing the gradients and\n",
    "applying them, so we need: <br>\n",
    "1. call the optimizer’s compute_gradients() method\n",
    "2. clip the gradients using the clip_by_value() function\n",
    "3. apply the clipped gradients using the optimizer’s apply_gradients() method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up network\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_hidden5 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we apply gradient clipping. For this, we need to get the gradients, use the `clip_by_value()` function to clip them, then apply them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "# Replace very large gradient values by threshold. \n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.2878\n",
      "1 Validation accuracy: 0.7942\n",
      "2 Validation accuracy: 0.8794\n",
      "3 Validation accuracy: 0.9054\n",
      "4 Validation accuracy: 0.9162\n",
      "5 Validation accuracy: 0.9212\n",
      "6 Validation accuracy: 0.9292\n",
      "7 Validation accuracy: 0.9358\n",
      "8 Validation accuracy: 0.9378\n",
      "9 Validation accuracy: 0.9414\n",
      "10 Validation accuracy: 0.9456\n",
      "11 Validation accuracy: 0.9472\n",
      "12 Validation accuracy: 0.9476\n",
      "13 Validation accuracy: 0.9536\n",
      "14 Validation accuracy: 0.9564\n",
      "15 Validation accuracy: 0.9566\n",
      "16 Validation accuracy: 0.9574\n",
      "17 Validation accuracy: 0.9588\n",
      "18 Validation accuracy: 0.9624\n",
      "19 Validation accuracy: 0.9616\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "# We save our checkpoint\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Pretrained Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally not a good idea to train a very large DNN from scratch: instead, you should always try to\n",
    "find an existing neural network that accomplishes a similar task to the one you are trying to tackle, then\n",
    "just reuse the lower layers of this network: this is called transfer learning.  <br>\n",
    "    \n",
    "It will not only speed up training considerably, but will also require much less training data. <br>\n",
    "\n",
    "For example, suppose that you have access to a DNN that was trained to classify pictures into 100\n",
    "different categories, including animals, plants, vehicles, and everyday objects. You now want to train a\n",
    "DNN to classify specific types of vehicles. These tasks are very similar, so you should try to reuse parts\n",
    "of the first network. <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"images\\Lesson10\\image1.png\" width=\"600\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing a TensorFlow Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you need to load the graph's structure. The `import_meta_graph()` function does just that, loading the graph's operations into the default graph, and returning a `Saver` that you can then use to restore the model's state. Note that by default, a `Saver` saves the structure of the graph into a `.meta` file, so that's the file you should load:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next you need to get a handle on all the operations you will need for training. If you don't know the graph's structure, you can list all the operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X\n",
      "y\n",
      "hidden1/kernel/Initializer/random_uniform/shape\n",
      "hidden1/kernel/Initializer/random_uniform/min\n",
      "hidden1/kernel/Initializer/random_uniform/max\n",
      "hidden1/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden1/kernel/Initializer/random_uniform/sub\n",
      "hidden1/kernel/Initializer/random_uniform/mul\n",
      "hidden1/kernel/Initializer/random_uniform\n",
      "hidden1/kernel\n",
      "hidden1/kernel/Assign\n",
      "hidden1/kernel/read\n",
      "hidden1/bias/Initializer/zeros\n",
      "hidden1/bias\n",
      "hidden1/bias/Assign\n",
      "hidden1/bias/read\n",
      "dnn/hidden1/MatMul\n",
      "dnn/hidden1/BiasAdd\n",
      "dnn/hidden1/Relu\n",
      "hidden2/kernel/Initializer/random_uniform/shape\n",
      "hidden2/kernel/Initializer/random_uniform/min\n",
      "hidden2/kernel/Initializer/random_uniform/max\n",
      "hidden2/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden2/kernel/Initializer/random_uniform/sub\n",
      "hidden2/kernel/Initializer/random_uniform/mul\n",
      "hidden2/kernel/Initializer/random_uniform\n",
      "hidden2/kernel\n",
      "hidden2/kernel/Assign\n",
      "hidden2/kernel/read\n",
      "hidden2/bias/Initializer/zeros\n",
      "hidden2/bias\n",
      "hidden2/bias/Assign\n",
      "hidden2/bias/read\n",
      "dnn/hidden2/MatMul\n",
      "dnn/hidden2/BiasAdd\n",
      "dnn/hidden2/Relu\n",
      "hidden3/kernel/Initializer/random_uniform/shape\n",
      "hidden3/kernel/Initializer/random_uniform/min\n",
      "hidden3/kernel/Initializer/random_uniform/max\n",
      "hidden3/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden3/kernel/Initializer/random_uniform/sub\n",
      "hidden3/kernel/Initializer/random_uniform/mul\n",
      "hidden3/kernel/Initializer/random_uniform\n",
      "hidden3/kernel\n",
      "hidden3/kernel/Assign\n",
      "hidden3/kernel/read\n",
      "hidden3/bias/Initializer/zeros\n",
      "hidden3/bias\n",
      "hidden3/bias/Assign\n",
      "hidden3/bias/read\n",
      "dnn/hidden3/MatMul\n",
      "dnn/hidden3/BiasAdd\n",
      "dnn/hidden3/Relu\n",
      "hidden4/kernel/Initializer/random_uniform/shape\n",
      "hidden4/kernel/Initializer/random_uniform/min\n",
      "hidden4/kernel/Initializer/random_uniform/max\n",
      "hidden4/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden4/kernel/Initializer/random_uniform/sub\n",
      "hidden4/kernel/Initializer/random_uniform/mul\n",
      "hidden4/kernel/Initializer/random_uniform\n",
      "hidden4/kernel\n",
      "hidden4/kernel/Assign\n",
      "hidden4/kernel/read\n",
      "hidden4/bias/Initializer/zeros\n",
      "hidden4/bias\n",
      "hidden4/bias/Assign\n",
      "hidden4/bias/read\n",
      "dnn/hidden4/MatMul\n",
      "dnn/hidden4/BiasAdd\n",
      "dnn/hidden4/Relu\n",
      "hidden5/kernel/Initializer/random_uniform/shape\n",
      "hidden5/kernel/Initializer/random_uniform/min\n",
      "hidden5/kernel/Initializer/random_uniform/max\n",
      "hidden5/kernel/Initializer/random_uniform/RandomUniform\n",
      "hidden5/kernel/Initializer/random_uniform/sub\n",
      "hidden5/kernel/Initializer/random_uniform/mul\n",
      "hidden5/kernel/Initializer/random_uniform\n",
      "hidden5/kernel\n",
      "hidden5/kernel/Assign\n",
      "hidden5/kernel/read\n",
      "hidden5/bias/Initializer/zeros\n",
      "hidden5/bias\n",
      "hidden5/bias/Assign\n",
      "hidden5/bias/read\n",
      "dnn/hidden5/MatMul\n",
      "dnn/hidden5/BiasAdd\n",
      "dnn/hidden5/Relu\n",
      "outputs/kernel/Initializer/random_uniform/shape\n",
      "outputs/kernel/Initializer/random_uniform/min\n",
      "outputs/kernel/Initializer/random_uniform/max\n",
      "outputs/kernel/Initializer/random_uniform/RandomUniform\n",
      "outputs/kernel/Initializer/random_uniform/sub\n",
      "outputs/kernel/Initializer/random_uniform/mul\n",
      "outputs/kernel/Initializer/random_uniform\n",
      "outputs/kernel\n",
      "outputs/kernel/Assign\n",
      "outputs/kernel/read\n",
      "outputs/bias/Initializer/zeros\n",
      "outputs/bias\n",
      "outputs/bias/Assign\n",
      "outputs/bias/read\n",
      "dnn/outputs/MatMul\n",
      "dnn/outputs/BiasAdd\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/Shape\n",
      "loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits\n",
      "loss/Const\n",
      "loss/loss\n",
      "gradients/Shape\n",
      "gradients/grad_ys_0\n",
      "gradients/Fill\n",
      "gradients/loss/loss_grad/Reshape/shape\n",
      "gradients/loss/loss_grad/Reshape\n",
      "gradients/loss/loss_grad/Shape\n",
      "gradients/loss/loss_grad/Tile\n",
      "gradients/loss/loss_grad/Shape_1\n",
      "gradients/loss/loss_grad/Shape_2\n",
      "gradients/loss/loss_grad/Const\n",
      "gradients/loss/loss_grad/Prod\n",
      "gradients/loss/loss_grad/Const_1\n",
      "gradients/loss/loss_grad/Prod_1\n",
      "gradients/loss/loss_grad/Maximum/y\n",
      "gradients/loss/loss_grad/Maximum\n",
      "gradients/loss/loss_grad/floordiv\n",
      "gradients/loss/loss_grad/Cast\n",
      "gradients/loss/loss_grad/truediv\n",
      "gradients/zeros_like\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims\n",
      "gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul\n",
      "gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul\n",
      "gradients/dnn/outputs/MatMul_grad/MatMul_1\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden5/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden4/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden3/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden2/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/Relu_grad/ReluGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul\n",
      "gradients/dnn/hidden1/MatMul_grad/MatMul_1\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/group_deps\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency\n",
      "gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1\n",
      "clip_by_value/Minimum/y\n",
      "clip_by_value/Minimum\n",
      "clip_by_value/y\n",
      "clip_by_value\n",
      "clip_by_value_1/Minimum/y\n",
      "clip_by_value_1/Minimum\n",
      "clip_by_value_1/y\n",
      "clip_by_value_1\n",
      "clip_by_value_2/Minimum/y\n",
      "clip_by_value_2/Minimum\n",
      "clip_by_value_2/y\n",
      "clip_by_value_2\n",
      "clip_by_value_3/Minimum/y\n",
      "clip_by_value_3/Minimum\n",
      "clip_by_value_3/y\n",
      "clip_by_value_3\n",
      "clip_by_value_4/Minimum/y\n",
      "clip_by_value_4/Minimum\n",
      "clip_by_value_4/y\n",
      "clip_by_value_4\n",
      "clip_by_value_5/Minimum/y\n",
      "clip_by_value_5/Minimum\n",
      "clip_by_value_5/y\n",
      "clip_by_value_5\n",
      "clip_by_value_6/Minimum/y\n",
      "clip_by_value_6/Minimum\n",
      "clip_by_value_6/y\n",
      "clip_by_value_6\n",
      "clip_by_value_7/Minimum/y\n",
      "clip_by_value_7/Minimum\n",
      "clip_by_value_7/y\n",
      "clip_by_value_7\n",
      "clip_by_value_8/Minimum/y\n",
      "clip_by_value_8/Minimum\n",
      "clip_by_value_8/y\n",
      "clip_by_value_8\n",
      "clip_by_value_9/Minimum/y\n",
      "clip_by_value_9/Minimum\n",
      "clip_by_value_9/y\n",
      "clip_by_value_9\n",
      "clip_by_value_10/Minimum/y\n",
      "clip_by_value_10/Minimum\n",
      "clip_by_value_10/y\n",
      "clip_by_value_10\n",
      "clip_by_value_11/Minimum/y\n",
      "clip_by_value_11/Minimum\n",
      "clip_by_value_11/y\n",
      "clip_by_value_11\n",
      "GradientDescent/learning_rate\n",
      "GradientDescent/update_hidden1/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden1/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden2/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden3/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden4/bias/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_hidden5/bias/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/kernel/ApplyGradientDescent\n",
      "GradientDescent/update_outputs/bias/ApplyGradientDescent\n",
      "GradientDescent\n",
      "eval/in_top_k/InTopKV2/k\n",
      "eval/in_top_k/InTopKV2\n",
      "eval/Cast\n",
      "eval/Const\n",
      "eval/accuracy\n",
      "init\n",
      "save/Const\n",
      "save/SaveV2/tensor_names\n",
      "save/SaveV2/shape_and_slices\n",
      "save/SaveV2\n",
      "save/control_dependency\n",
      "save/RestoreV2/tensor_names\n",
      "save/RestoreV2/shape_and_slices\n",
      "save/RestoreV2\n",
      "save/Assign\n",
      "save/Assign_1\n",
      "save/Assign_2\n",
      "save/Assign_3\n",
      "save/Assign_4\n",
      "save/Assign_5\n",
      "save/Assign_6\n",
      "save/Assign_7\n",
      "save/Assign_8\n",
      "save/Assign_9\n",
      "save/Assign_10\n",
      "save/Assign_11\n",
      "save/restore_all\n"
     ]
    }
   ],
   "source": [
    "for op in tf.get_default_graph().get_operations():\n",
    "    print(op.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oops, that's a lot of operations! It's much easier to use TensorBoard to visualize the graph. The following hack will allow you to visualize the graph within Jupyter (if it does not work with your browser, you will need to use a `FileWriter` to save the graph and then visualize it in TensorBoard):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_graph_in_jupyter import show_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe seamless style=\"width:1200px;height:620px;border:0\" srcdoc=\"\n",
       "        <script src=&quot;//cdnjs.cloudflare.com/ajax/libs/polymer/0.3.3/platform.js&quot;></script>\n",
       "        <script>\n",
       "          function load() {\n",
       "            document.getElementById(&quot;graph0.3745401188473625&quot;).pbtxt = 'node {\\n  name: &quot;X&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: -1\\n        }\\n        dim {\\n          size: 784\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;y&quot;\\n  op: &quot;Placeholder&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        unknown_rank: true\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;\\\\020\\\\003\\\\000\\\\000,\\\\001\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.07439795136451721\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 5\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 784\\n        }\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden1/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 300\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden1/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden1/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden1/MatMul&quot;\\n  input: &quot;hidden1/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden1/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden1/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;,\\\\001\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.13093073666095734\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 22\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 300\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden2/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden2/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden2/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden2/MatMul&quot;\\n  input: &quot;hidden2/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden2/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden2/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 39\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden3/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden3/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden3/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden3/MatMul&quot;\\n  input: &quot;hidden3/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden3/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden3/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 56\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden4/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden4/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden4/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden4/MatMul&quot;\\n  input: &quot;hidden4/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden4/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden4/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\0002\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.24494896829128265\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 73\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;hidden5/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 50\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;hidden5/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;hidden5/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/hidden5/MatMul&quot;\\n  input: &quot;hidden5/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/hidden5/Relu&quot;\\n  op: &quot;Relu&quot;\\n  input: &quot;dnn/hidden5/BiasAdd&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 2\\n          }\\n        }\\n        tensor_content: &quot;2\\\\000\\\\000\\\\000\\\\n\\\\000\\\\000\\\\000&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.3162277638912201\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  op: &quot;RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;seed&quot;\\n    value {\\n      i: 42\\n    }\\n  }\\n  attr {\\n    key: &quot;seed2&quot;\\n    value {\\n      i: 90\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  op: &quot;Sub&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/max&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/RandomUniform&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/sub&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  op: &quot;Add&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/mul&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform/min&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 50\\n        }\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;outputs/kernel/Initializer/random_uniform&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/kernel/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Initializer/zeros&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n          dim {\\n            size: 10\\n          }\\n        }\\n        float_val: 0.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias&quot;\\n  op: &quot;VariableV2&quot;\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;container&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;shape&quot;\\n    value {\\n      shape {\\n        dim {\\n          size: 10\\n        }\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;shared_name&quot;\\n    value {\\n      s: &quot;&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/bias/Initializer/zeros&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;outputs/bias/read&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;outputs/bias&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;dnn/outputs/BiasAdd&quot;\\n  op: &quot;BiasAdd&quot;\\n  input: &quot;dnn/outputs/MatMul&quot;\\n  input: &quot;outputs/bias/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  op: &quot;SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tlabels&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;loss/loss&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  input: &quot;loss/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/grad_ys_0&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/Fill&quot;\\n  op: &quot;Fill&quot;\\n  input: &quot;gradients/Shape&quot;\\n  input: &quot;gradients/grad_ys_0&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;index_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  op: &quot;Reshape&quot;\\n  input: &quot;gradients/Fill&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape/shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tshape&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Tile&quot;\\n  op: &quot;Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Reshape&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tmultiples&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  op: &quot;Shape&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;out_type&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n          }\\n        }\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  op: &quot;Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Shape_2&quot;\\n  input: &quot;gradients/loss/loss_grad/Const_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod_1&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  op: &quot;FloorDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Prod&quot;\\n  input: &quot;gradients/loss/loss_grad/Maximum&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;gradients/loss/loss_grad/floordiv&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/loss_grad/truediv&quot;\\n  op: &quot;RealDiv&quot;\\n  input: &quot;gradients/loss/loss_grad/Tile&quot;\\n  input: &quot;gradients/loss/loss_grad/Cast&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/zeros_like&quot;\\n  op: &quot;ZerosLike&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  op: &quot;PreventGradient&quot;\\n  input: &quot;loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;message&quot;\\n    value {\\n      s: &quot;Currently there is no way to take the second derivative of sparse_softmax_cross_entropy_with_logits due to the fused implementation\\\\\\'s interaction with tf.gradients()&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: -1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  op: &quot;ExpandDims&quot;\\n  input: &quot;gradients/loss/loss_grad/truediv&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims/dim&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tdim&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  op: &quot;Mul&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/ExpandDims&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/PreventGradient&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/loss/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits_grad/mul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/outputs/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;outputs/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/outputs/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/outputs/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden5/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden5/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden5/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden5/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden5/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden4/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden4/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden4/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden4/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden4/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden3/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden3/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden3/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden3/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden3/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden2/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden2/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden2/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden2/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden2/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  op: &quot;ReluGrad&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency&quot;\\n  input: &quot;dnn/hidden1/Relu&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  op: &quot;BiasAddGrad&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;data_format&quot;\\n    value {\\n      s: &quot;NHWC&quot;\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/Relu_grad/ReluGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n  input: &quot;^gradients/dnn/hidden1/BiasAdd_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/BiasAdd_grad/BiasAddGrad&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  input: &quot;hidden1/kernel/read&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  op: &quot;MatMul&quot;\\n  input: &quot;X&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_a&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;transpose_b&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n  input: &quot;^gradients/dnn/hidden1/MatMul_grad/tuple/group_deps&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@gradients/dnn/hidden1/MatMul_grad/MatMul_1&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value/Minimum&quot;\\n  input: &quot;clip_by_value/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden1/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_1/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_1&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_1/Minimum&quot;\\n  input: &quot;clip_by_value_1/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_2/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_2&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_2/Minimum&quot;\\n  input: &quot;clip_by_value_2/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden2/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_3/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_3&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_3/Minimum&quot;\\n  input: &quot;clip_by_value_3/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_4/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_4&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_4/Minimum&quot;\\n  input: &quot;clip_by_value_4/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden3/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_5/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_5&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_5/Minimum&quot;\\n  input: &quot;clip_by_value_5/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_6/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_6&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_6/Minimum&quot;\\n  input: &quot;clip_by_value_6/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden4/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_7/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_7&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_7/Minimum&quot;\\n  input: &quot;clip_by_value_7/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_8/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_8&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_8/Minimum&quot;\\n  input: &quot;clip_by_value_8/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/hidden5/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_9/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_9&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_9/Minimum&quot;\\n  input: &quot;clip_by_value_9/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/MatMul_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_10/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_10&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_10/Minimum&quot;\\n  input: &quot;clip_by_value_10/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/Minimum&quot;\\n  op: &quot;Minimum&quot;\\n  input: &quot;gradients/dnn/outputs/BiasAdd_grad/tuple/control_dependency_1&quot;\\n  input: &quot;clip_by_value_11/Minimum/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11/y&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: -1.0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;clip_by_value_11&quot;\\n  op: &quot;Maximum&quot;\\n  input: &quot;clip_by_value_11/Minimum&quot;\\n  input: &quot;clip_by_value_11/y&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/learning_rate&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_FLOAT\\n        tensor_shape {\\n        }\\n        float_val: 0.009999999776482582\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  op: &quot;ApplyGradientDescent&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;GradientDescent/learning_rate&quot;\\n  input: &quot;clip_by_value_11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;GradientDescent&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^GradientDescent/update_hidden1/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden1/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden2/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden3/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden4/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_hidden5/kernel/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/bias/ApplyGradientDescent&quot;\\n  input: &quot;^GradientDescent/update_outputs/kernel/ApplyGradientDescent&quot;\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n        }\\n        int_val: 1\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/in_top_k/InTopKV2&quot;\\n  op: &quot;InTopKV2&quot;\\n  input: &quot;dnn/outputs/BiasAdd&quot;\\n  input: &quot;y&quot;\\n  input: &quot;eval/in_top_k/InTopKV2/k&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Cast&quot;\\n  op: &quot;Cast&quot;\\n  input: &quot;eval/in_top_k/InTopKV2&quot;\\n  attr {\\n    key: &quot;DstT&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;SrcT&quot;\\n    value {\\n      type: DT_BOOL\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_INT32\\n        tensor_shape {\\n          dim {\\n            size: 1\\n          }\\n        }\\n        int_val: 0\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;eval/accuracy&quot;\\n  op: &quot;Mean&quot;\\n  input: &quot;eval/Cast&quot;\\n  input: &quot;eval/Const&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;Tidx&quot;\\n    value {\\n      type: DT_INT32\\n    }\\n  }\\n  attr {\\n    key: &quot;keep_dims&quot;\\n    value {\\n      b: false\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;init&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^hidden1/bias/Assign&quot;\\n  input: &quot;^hidden1/kernel/Assign&quot;\\n  input: &quot;^hidden2/bias/Assign&quot;\\n  input: &quot;^hidden2/kernel/Assign&quot;\\n  input: &quot;^hidden3/bias/Assign&quot;\\n  input: &quot;^hidden3/kernel/Assign&quot;\\n  input: &quot;^hidden4/bias/Assign&quot;\\n  input: &quot;^hidden4/kernel/Assign&quot;\\n  input: &quot;^hidden5/bias/Assign&quot;\\n  input: &quot;^hidden5/kernel/Assign&quot;\\n  input: &quot;^outputs/bias/Assign&quot;\\n  input: &quot;^outputs/kernel/Assign&quot;\\n}\\nnode {\\n  name: &quot;save/Const&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n        }\\n        string_val: &quot;model&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/SaveV2&quot;\\n  op: &quot;SaveV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/SaveV2/tensor_names&quot;\\n  input: &quot;save/SaveV2/shape_and_slices&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;outputs/kernel&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/control_dependency&quot;\\n  op: &quot;Identity&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;^save/SaveV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@save/Const&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/tensor_names&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;hidden1/bias&quot;\\n        string_val: &quot;hidden1/kernel&quot;\\n        string_val: &quot;hidden2/bias&quot;\\n        string_val: &quot;hidden2/kernel&quot;\\n        string_val: &quot;hidden3/bias&quot;\\n        string_val: &quot;hidden3/kernel&quot;\\n        string_val: &quot;hidden4/bias&quot;\\n        string_val: &quot;hidden4/kernel&quot;\\n        string_val: &quot;hidden5/bias&quot;\\n        string_val: &quot;hidden5/kernel&quot;\\n        string_val: &quot;outputs/bias&quot;\\n        string_val: &quot;outputs/kernel&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2/shape_and_slices&quot;\\n  op: &quot;Const&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtype&quot;\\n    value {\\n      type: DT_STRING\\n    }\\n  }\\n  attr {\\n    key: &quot;value&quot;\\n    value {\\n      tensor {\\n        dtype: DT_STRING\\n        tensor_shape {\\n          dim {\\n            size: 12\\n          }\\n        }\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n        string_val: &quot;&quot;\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/RestoreV2&quot;\\n  op: &quot;RestoreV2&quot;\\n  input: &quot;save/Const&quot;\\n  input: &quot;save/RestoreV2/tensor_names&quot;\\n  input: &quot;save/RestoreV2/shape_and_slices&quot;\\n  device: &quot;/device:CPU:0&quot;\\n  attr {\\n    key: &quot;dtypes&quot;\\n    value {\\n      list {\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n        type: DT_FLOAT\\n      }\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/bias&quot;\\n  input: &quot;save/RestoreV2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_1&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden1/kernel&quot;\\n  input: &quot;save/RestoreV2:1&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden1/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_2&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/bias&quot;\\n  input: &quot;save/RestoreV2:2&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_3&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden2/kernel&quot;\\n  input: &quot;save/RestoreV2:3&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden2/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_4&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/bias&quot;\\n  input: &quot;save/RestoreV2:4&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_5&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden3/kernel&quot;\\n  input: &quot;save/RestoreV2:5&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden3/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_6&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/bias&quot;\\n  input: &quot;save/RestoreV2:6&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_7&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden4/kernel&quot;\\n  input: &quot;save/RestoreV2:7&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden4/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_8&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/bias&quot;\\n  input: &quot;save/RestoreV2:8&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_9&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;hidden5/kernel&quot;\\n  input: &quot;save/RestoreV2:9&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@hidden5/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_10&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/bias&quot;\\n  input: &quot;save/RestoreV2:10&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/bias&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/Assign_11&quot;\\n  op: &quot;Assign&quot;\\n  input: &quot;outputs/kernel&quot;\\n  input: &quot;save/RestoreV2:11&quot;\\n  attr {\\n    key: &quot;T&quot;\\n    value {\\n      type: DT_FLOAT\\n    }\\n  }\\n  attr {\\n    key: &quot;_class&quot;\\n    value {\\n      list {\\n        s: &quot;loc:@outputs/kernel&quot;\\n      }\\n    }\\n  }\\n  attr {\\n    key: &quot;use_locking&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n  attr {\\n    key: &quot;validate_shape&quot;\\n    value {\\n      b: true\\n    }\\n  }\\n}\\nnode {\\n  name: &quot;save/restore_all&quot;\\n  op: &quot;NoOp&quot;\\n  input: &quot;^save/Assign&quot;\\n  input: &quot;^save/Assign_1&quot;\\n  input: &quot;^save/Assign_10&quot;\\n  input: &quot;^save/Assign_11&quot;\\n  input: &quot;^save/Assign_2&quot;\\n  input: &quot;^save/Assign_3&quot;\\n  input: &quot;^save/Assign_4&quot;\\n  input: &quot;^save/Assign_5&quot;\\n  input: &quot;^save/Assign_6&quot;\\n  input: &quot;^save/Assign_7&quot;\\n  input: &quot;^save/Assign_8&quot;\\n  input: &quot;^save/Assign_9&quot;\\n}\\n';\n",
       "          }\n",
       "        </script>\n",
       "        <link rel=&quot;import&quot; href=&quot;https://tensorboard.appspot.com/tf-graph-basic.build.html&quot; onload=load()>\n",
       "        <div style=&quot;height:600px&quot;>\n",
       "          <tf-graph-basic id=&quot;graph0.3745401188473625&quot;></tf-graph-basic>\n",
       "        </div>\n",
       "    \"></iframe>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_graph(tf.get_default_graph())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you know which operations you need, you can get a handle on them using the graph's `get_operation_by_name()` or `get_tensor_by_name()` methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"eval/accuracy:0\")\n",
    "\n",
    "training_op = tf.get_default_graph().get_operation_by_name(\"GradientDescent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are the author of the original model, you could make things easier for people who will reuse your model by giving operations very clear names and documenting them. Another approach is to create a collection containing all the important operations that people will want to get a handle on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "for op in (X, y, accuracy, training_op):\n",
    "    tf.add_to_collection(\"my_important_ops\", op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This way people who reuse your model will be able to simply write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, accuracy, training_op = tf.get_collection(\"my_important_ops\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can start a session, restore the model's state and continue training on your data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n"
     ]
    }
   ],
   "source": [
    "# restore the checkpoint\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # continue training the model..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, let's test this for real!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9632\n",
      "1 Validation accuracy: 0.9632\n",
      "2 Validation accuracy: 0.9656\n",
      "3 Validation accuracy: 0.965\n",
      "4 Validation accuracy: 0.9642\n",
      "5 Validation accuracy: 0.965\n",
      "6 Validation accuracy: 0.9686\n",
      "7 Validation accuracy: 0.9686\n",
      "8 Validation accuracy: 0.9684\n",
      "9 Validation accuracy: 0.9684\n",
      "10 Validation accuracy: 0.9706\n",
      "11 Validation accuracy: 0.9716\n",
      "12 Validation accuracy: 0.967\n",
      "13 Validation accuracy: 0.9706\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9724\n",
      "16 Validation accuracy: 0.972\n",
      "17 Validation accuracy: 0.9714\n",
      "18 Validation accuracy: 0.9714\n",
      "19 Validation accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\") \n",
    "# The improvement is small, most of the model remain the same."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if you have access to the Python code that built the original graph, you can use it instead of `import_meta_graph()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_hidden3 = 50\n",
    "n_hidden4 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\")\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\")\n",
    "    hidden5 = tf.layers.dense(hidden4, n_hidden5, activation=tf.nn.relu, name=\"hidden5\")\n",
    "    logits = tf.layers.dense(hidden5, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "threshold = 1.0\n",
    "\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var)\n",
    "              for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "# Save the model\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And continue training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9642\n",
      "1 Validation accuracy: 0.9628\n",
      "2 Validation accuracy: 0.9654\n",
      "3 Validation accuracy: 0.9652\n",
      "4 Validation accuracy: 0.9642\n",
      "5 Validation accuracy: 0.9648\n",
      "6 Validation accuracy: 0.969\n",
      "7 Validation accuracy: 0.9686\n",
      "8 Validation accuracy: 0.9684\n",
      "9 Validation accuracy: 0.9684\n",
      "10 Validation accuracy: 0.9704\n",
      "11 Validation accuracy: 0.9714\n",
      "12 Validation accuracy: 0.9672\n",
      "13 Validation accuracy: 0.9702\n",
      "14 Validation accuracy: 0.9712\n",
      "15 Validation accuracy: 0.9724\n",
      "16 Validation accuracy: 0.972\n",
      "17 Validation accuracy: 0.971\n",
      "18 Validation accuracy: 0.9714\n",
      "19 Validation accuracy: 0.9714\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general you will want to reuse only the lower layers. If you are using `import_meta_graph()` it will load the whole graph, but you can simply ignore the parts you do not need. In this example, we add a new 4th hidden layer on top of the pretrained 3rd layer (ignoring the old 4th hidden layer). We also build a new output layer, the loss for this new output, and a new optimizer to minimize it. We also need another saver to save the whole graph (containing both the entire old graph plus the new operations), and an initialization operation to initialize all the new variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_hidden4 = 20  # new layer\n",
    "n_outputs = 10  # new layer\n",
    "\n",
    "saver = tf.train.import_meta_graph(\"./my_model_final.ckpt.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "# Load a third layer as old hidden4 from the Relu model. \n",
    "hidden3 = tf.get_default_graph().get_tensor_by_name(\"dnn/hidden4/Relu:0\")\n",
    "\n",
    "new_hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"new_hidden4\")\n",
    "new_logits = tf.layers.dense(new_hidden4, n_outputs, name=\"new_outputs\")\n",
    "\n",
    "with tf.name_scope(\"new_loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=new_logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"new_eval\"):\n",
    "    correct = tf.nn.in_top_k(new_logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"new_train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can train this new model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9372\n",
      "1 Validation accuracy: 0.9522\n",
      "2 Validation accuracy: 0.9566\n",
      "3 Validation accuracy: 0.9572\n",
      "4 Validation accuracy: 0.9572\n",
      "5 Validation accuracy: 0.9582\n",
      "6 Validation accuracy: 0.9608\n",
      "7 Validation accuracy: 0.964\n",
      "8 Validation accuracy: 0.9638\n",
      "9 Validation accuracy: 0.9636\n",
      "10 Validation accuracy: 0.963\n",
      "11 Validation accuracy: 0.965\n",
      "12 Validation accuracy: 0.9616\n",
      "13 Validation accuracy: 0.9658\n",
      "14 Validation accuracy: 0.9676\n",
      "15 Validation accuracy: 0.9676\n",
      "16 Validation accuracy: 0.9692\n",
      "17 Validation accuracy: 0.9676\n",
      "18 Validation accuracy: 0.97\n",
      "19 Validation accuracy: 0.9676\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = new_saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have access to the Python code that built the original graph, you can just reuse the parts you need and drop the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, you must create one `Saver` to restore the pretrained model (giving it the list of variables to restore, or else it will complain that the graphs don't match), and another `Saver` to save the new model, once it is trained: <br>\n",
    "\n",
    "However, in general you will want to reuse only part of the original model (as we will discuss in a moment). A simple solution is to configure the Saver to restore only a subset of the variables from the original model. For example, the following code restores only hidden layers 1, 2, and 3: <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9018\n",
      "1 Validation accuracy: 0.9324\n",
      "2 Validation accuracy: 0.9432\n",
      "3 Validation accuracy: 0.947\n",
      "4 Validation accuracy: 0.9516\n",
      "5 Validation accuracy: 0.9532\n",
      "6 Validation accuracy: 0.956\n",
      "7 Validation accuracy: 0.959\n",
      "8 Validation accuracy: 0.9586\n",
      "9 Validation accuracy: 0.9612\n",
      "10 Validation accuracy: 0.9628\n",
      "11 Validation accuracy: 0.9618\n",
      "12 Validation accuracy: 0.9638\n",
      "13 Validation accuracy: 0.9658\n",
      "14 Validation accuracy: 0.9664\n",
      "15 Validation accuracy: 0.9662\n",
      "16 Validation accuracy: 0.9674\n",
      "17 Validation accuracy: 0.9676\n",
      "18 Validation accuracy: 0.968\n",
      "19 Validation accuracy: 0.9674\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):                                            # not shown in the book\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})        # not shown\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})     # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)                   # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we build the new model, making sure to copy the original model’s hidden layers 1 to 3. We also create a node to initialize all variables. Then we get the list of all variables that were just created with \"trainable=True\" (which is the default), and we keep only the ones whose scope matches the regular expression \"hidden[123]\" (i.e., we get all trainable variables in hidden layers 1 to 3). Next we create a dictionary mapping the name of each variable in the original model to its name in the new model\n",
    "(generally you want to keep the exact same names). Then we create a Saver that will restore only these variables, and we create another Saver to save the entire new model, not just layers 1 to 3. We then start a session and initialize all variables in the model, then restore the variable values from the original model’s layers 1 to 3. Finally, we train the model on the new task and save it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Freezing the Lower Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is likely that the lower layers of the first DNN have learned to detect low-level features in pictures that will be useful across both image classification tasks, so you can just reuse these layers as they are. <br>\n",
    "\n",
    "It is generally a good idea to “freeze” their weights when training the new DNN: if the lower-layer weights are fixed, then the higher-layer weights will be easier to train (because they won’t have to learn a moving target).\n",
    "\n",
    "To freeze the lower layers during training, the simplest solution is to give the optimizer the list of variables to train, excluding the variables from the lower layers.\n",
    "\n",
    "We start with the general model:\n",
    "\n",
    "The model with frozen layers has faster convergence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")       # reused\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\") # reused\n",
    "    hidden3 = tf.layers.dense(hidden2, n_hidden3, activation=tf.nn.relu, name=\"hidden3\") # reused\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu, name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\")                         # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first line gets the list of all trainable variables in hidden layers 3 and 4 and in the output layer. This leaves out the variables in the hidden layers 1 and 2. \n",
    "\n",
    "Next we provide this restricted list of trainable variables to the optimizer’s minimize() function. Ta-da! Layers 1 and 2 are now frozen: they will not budge during training (these are often called frozen layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify learning using only layers 3 and 4.\n",
    "with tf.name_scope(\"train\"):                                         # not shown in the book\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)     # not shown\n",
    "    train_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,\n",
    "                                   scope=\"hidden[34]|outputs\")\n",
    "    training_op = optimizer.minimize(loss, var_list=train_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "new_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.896\n",
      "1 Validation accuracy: 0.9304\n",
      "2 Validation accuracy: 0.9398\n",
      "3 Validation accuracy: 0.9446\n",
      "4 Validation accuracy: 0.9482\n",
      "5 Validation accuracy: 0.9504\n",
      "6 Validation accuracy: 0.9506\n",
      "7 Validation accuracy: 0.9536\n",
      "8 Validation accuracy: 0.9552\n",
      "9 Validation accuracy: 0.9564\n",
      "10 Validation accuracy: 0.956\n",
      "11 Validation accuracy: 0.9566\n",
      "12 Validation accuracy: 0.9568\n",
      "13 Validation accuracy: 0.9572\n",
      "14 Validation accuracy: 0.959\n",
      "15 Validation accuracy: 0.9576\n",
      "16 Validation accuracy: 0.9578\n",
      "17 Validation accuracy: 0.9598\n",
      "18 Validation accuracy: 0.959\n",
      "19 Validation accuracy: 0.9598\n"
     ]
    }
   ],
   "source": [
    "# First estimate 3 hidden layers\n",
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "# Restore final model checkpoint\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "# Train only layers 3 and 4, other layers are frozen.\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen\n",
    "    # We transfer fixed gradients to the layer 3. Now layers 1 and 2 cannot be changed. \n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training code is exactly the same as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9306\n",
      "2 Validation accuracy: 0.9434\n",
      "3 Validation accuracy: 0.9474\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9524\n",
      "7 Validation accuracy: 0.9556\n",
      "8 Validation accuracy: 0.9554\n",
      "9 Validation accuracy: 0.956\n",
      "10 Validation accuracy: 0.9572\n",
      "11 Validation accuracy: 0.955\n",
      "12 Validation accuracy: 0.9574\n",
      "13 Validation accuracy: 0.958\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.958\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9576\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caching the Frozen Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the frozen layers won’t change, it is possible to cache the output of the topmost frozen layer for each training instance.\n",
    "\n",
    "Since training goes through the whole dataset many times, this will give you a huge speed boost as you will only need to go through the frozen layers once per training instance (instead of once per epoch). For example, you could first run the whole training set through the lower layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same code as above\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300 # reused\n",
    "n_hidden2 = 50  # reused\n",
    "n_hidden3 = 50  # reused\n",
    "n_hidden4 = 20  # new!\n",
    "n_outputs = 10  # new!\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\") # reused frozen\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\") # reused frozen & cached\n",
    "    hidden2_stop = tf.stop_gradient(hidden2)\n",
    "    hidden3 = tf.layers.dense(hidden2_stop, n_hidden3, activation=tf.nn.relu,\n",
    "                              name=\"hidden3\") # reused, not frozen\n",
    "    hidden4 = tf.layers.dense(hidden3, n_hidden4, activation=tf.nn.relu,\n",
    "                              name=\"hidden4\") # new!\n",
    "    logits = tf.layers.dense(hidden4, n_outputs, name=\"outputs\") # new!\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "reuse_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES,\n",
    "                               scope=\"hidden[123]\") # regular expression\n",
    "restore_saver = tf.train.Saver(reuse_vars) # to restore layers 1-3\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./my_model_final.ckpt\n",
      "0 Validation accuracy: 0.9022\n",
      "1 Validation accuracy: 0.9306\n",
      "2 Validation accuracy: 0.9434\n",
      "3 Validation accuracy: 0.9474\n",
      "4 Validation accuracy: 0.9514\n",
      "5 Validation accuracy: 0.9522\n",
      "6 Validation accuracy: 0.9524\n",
      "7 Validation accuracy: 0.9556\n",
      "8 Validation accuracy: 0.9554\n",
      "9 Validation accuracy: 0.956\n",
      "10 Validation accuracy: 0.9572\n",
      "11 Validation accuracy: 0.955\n",
      "12 Validation accuracy: 0.9574\n",
      "13 Validation accuracy: 0.958\n",
      "14 Validation accuracy: 0.9582\n",
      "15 Validation accuracy: 0.958\n",
      "16 Validation accuracy: 0.9566\n",
      "17 Validation accuracy: 0.9576\n",
      "18 Validation accuracy: 0.9592\n",
      "19 Validation accuracy: 0.9588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n_batches = len(X_train) // batch_size\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_model_final.ckpt\")\n",
    "    # We will run it for a single epoch and save\n",
    "    h2_cache = sess.run(hidden2, feed_dict={X: X_train})\n",
    "    h2_cache_valid = sess.run(hidden2, feed_dict={X: X_valid}) # not shown in the book\n",
    "    # The rest will be reused.\n",
    "    for epoch in range(n_epochs):\n",
    "        shuffled_idx = np.random.permutation(len(X_train))\n",
    "        # Reuse\n",
    "        hidden2_batches = np.array_split(h2_cache[shuffled_idx], n_batches)\n",
    "        y_batches = np.array_split(y_train[shuffled_idx], n_batches)\n",
    "        for hidden2_batch, y_batch in zip(hidden2_batches, y_batches):\n",
    "            sess.run(training_op, feed_dict={hidden2:hidden2_batch, y:y_batch})\n",
    "\n",
    "        accuracy_val = accuracy.eval(feed_dict={hidden2: h2_cache_valid, # not shown\n",
    "                                                y: y_valid})             # not shown\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_new_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tweaking, Dropping, or Replacing the Upper Layers\n",
    "\n",
    "1. The output layer of the original model should usually be replaced since it is most likely not useful at all for the new task, and it may not even have the right number of outputs for the new task.\n",
    "2. the upper hidden layers of the original model are less likely to be as useful as the lower layers, since the high-level features that are most useful for the new task may differ significantly from the ones that were most useful for the original task.\n",
    "3. Try freezing all the copied layers first, then train your model and see how it performs.\n",
    "4. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze.\n",
    "5. If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freeze all remaining hidden layers again.\n",
    "6. You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even add more hidden layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Zoos\n",
    "Where can you find a neural network trained for a task similar to the one you want to tackle? The first place to look is obviously in your own catalog of models. This is one good reason to save all your models and organize them so you can retrieve them later easily. Another option is to search in a model zoo. Many people train Machine Learning models for various tasks and kindly release their pretrained models to the public."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Faster Optimizers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a very large deep neural network can be painfully slow. So far we have seen four ways to speed up training:\n",
    "\n",
    "1. applying a good initialization strategy for the connection weights\n",
    "2. using a good activation function\n",
    "3. using Batch Normalization/SELU\n",
    "4. reusing parts of a pretrained network. \n",
    "\n",
    "Another huge speed boost comes from using a faster optimizer than the regular Gradient Descent optimizer.\n",
    "\n",
    "\n",
    "Momentum optimization, Nesterov Accelerated Gradient, AdaGrad, RMSProp, and finally Adam optimization.\n",
    " \n",
    "Adam optimization is usally the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Momentum optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "magine a bowling ball rolling down a gentle slope on a smooth surface: it will start out slowly, but it will quickly pick up momentum until it eventually reaches terminal velocity (if there is some friction or air resistance).\n",
    "\n",
    "In contrast, regular Gradient Descent will simply take small regular steps down the slope, so it will take much more time to reach the bottom.\n",
    "\n",
    "Gradient Descent simply updates the weights $\\theta$ by directly subtracting the gradient of the cost function $J(\\theta)$ with regards to the weights $(\\nabla\\theta J(\\theta))$multiplied by the learning rate $\\eta$. The equation is: $\\theta=\\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta)$. \n",
    "\n",
    "It does not care about what the earlier gradients were. If the local gradient is tiny, it goes very slowly.\n",
    "\n",
    "\n",
    "Momentum optimization cares a great deal about what previous gradients were: at each iteration, it adds the local gradient to the momentum vector $m$ (multiplied by the learning rate $\\eta$), and it updates the weights by simply subtracting this momentum vector.\n",
    "\n",
    "In other words, the gradient is used as an acceleration, not as a speed. To simulate some sort of friction mechanism and prevent the momentum from growing too large, the algorithm introduces a new hyperparameter $\\beta$, simply called the momentum, which must be set between 0 (high friction) and 1 (no friction). A typical momentum value is 0.9.\n",
    "\n",
    "$$m = \\beta m + \\eta \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the gradient remains constant then the maximum size of weight updates is\n",
    "$$m(1-\\beta)= \\eta \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "For $\\beta = 0.9$, we reach terminal velocity when $m = 10 * \\eta \\nabla_{\\theta} \\theta J(\\theta)$. It means that momentum optimization will be 10 times faster than the gradient descent. Momentum is especially fast when dealing with flat-gradients and it helps not to get stuck in the local optima."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nesterov Accelerated Gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modification of Momentum optimization, proposed by Yurii Nesterov in 1983, is almost always faster than vanilla Momentum optimization.\n",
    "\n",
    "The idea of Nesterov Momentum optimization, or Nesterov Accelerated Gradient (NAG) measures the gradient of the cost function not at the local position but slightly ahead in the direction of the momentum. The only difference from vanilla Momentum optimization is that the gradient is measured at $\\theta + \\beta m$ rather than at $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$m = \\beta m - \\eta \\nabla_{\\theta} \\theta J(\\theta + \\beta m)$$\n",
    "$$\\theta = \\theta + m$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This small tweak works because in general the momentum vector will be pointing in the right direction (i.e., toward the optimum), so it will be slightly more accurate to use the gradient measured a bit farther in that direction rather than using the gradient at the original position,\n",
    "\n",
    "\n",
    "$\\nabla_1$ is the gradient of the cost function measured at the starting point $\\theta$, and $\\nabla_2$ is the gradient at the point located at $\\theta + \\beta m$). \n",
    "\n",
    "The Nesterov update ends up slightly closer to the optimum. After a while, these small improvements add up and NAG ends up being significantly faster than regular Momentum optimization. \n",
    "\n",
    "Moreover, note that when the momentum pushes the weights across a valley, $\\nabla_1$ continues to push further across the valley, while $\\nabla_2$ pushes back toward the bottom of the valley. This helps reduce oscillations and thus converges faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"images\\Lesson10\\image2.png\" width=\"600\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.MomentumOptimizer(learning_rate=learning_rate,\n",
    "                                       momentum=0.9, use_nesterov=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaGrad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the elongated bowl problem again: Gradient Descent starts by quickly going down the steepest slope, then slowly goes down the bottom of the valley. \n",
    "\n",
    "It would be nice if the algorithm could detect this early on and correct its direction to point a bit more toward the global optimum. The AdaGrad algorithm achieves this by scaling down the gradient vector along the steepest dimensions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$s=s + \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step accumulates the square of the gradients into the vector $s$ (the $\\otimes$  symbol represents the element-wise multiplication). \n",
    "\n",
    "This vectorized form is equivalent to computing $s_i = s_i + (\\partial/\\partial \\theta_i J(\\theta))^2$ for each element $s_i$ of the vector $s$. \n",
    "Each $s_i$ accumulates the squares of the partial derivative of the cost function with regards to parameter $\\theta_i$. If the cost function is steep along the ith dimension, then $s_i$ will get larger and larger at each iteration.\n",
    "\n",
    "The second step is almost identical to Gradient Descent, but with one big difference: the gradient vector $s$ is scaled down by a factor of $\\sqrt{s+\\epsilon}$. $\\oslash$ is the element-by-element division, and $\\epsilon$ is the smoothing term to avoid division by zero set to $10^{-10}$. This is equivalent to element-wise:\n",
    "$$\\theta_i = \\theta_i - \\eta \\partial/\\partial \\theta_i J(\\theta)/\\sqrt{s+\\epsilon}$$\n",
    "\n",
    "\n",
    "In short, this algorithm decays the learning rate, but it does so faster for steep dimensions than for dimensions with gentler slopes. This is called an adaptive learning rate. It helps point the resulting updates more directly toward the global optimum  One additional benefit is that it requires much less tuning of the learning rate hyper-parameter $\\eta$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "    <img src=\"images\\Lesson10\\image3.png\" width=\"600\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaGrad often performs well for simple quadratic problems, but unfortunately it often stops too early when training neural networks. The learning rate gets scaled down so much that the algorithm ends up stopping entirely before reaching the global optimum. So even though TensorFlow has an AdagradOptimizer, you should not use it to train deep neural networks (it may be efficient for simpler tasks such as Linear Regression, though)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMSProp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although AdaGrad slows down a bit too fast and ends up never converging to the global optimum, the RMSProp algorithm fixes this by accumulating only the gradients from the most recent iterations (as opposed to all the gradients since the beginning of training).\n",
    "\n",
    "It does so by using exponential decay in the first step:\n",
    "\n",
    "$$s=\\beta s + (1-\\beta) \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - \\eta \\nabla_{\\theta} \\theta J(\\theta) \\oslash \\sqrt{s + \\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate,\n",
    "                                      momentum=0.9, decay=0.9, epsilon=1e-10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adam Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam is Adaptive Moment Estimation, combines the ideas of Momentum optimization and RMSProp:\n",
    "1.Like Momentum optimization it keeps track of an exponentially decaying average of past gradients (direction)\n",
    "2.Like RMSProp it keeps track of an exponentially decaying average of past squared gradients (size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. $m = \\beta_1 m + (1- \\beta_1) \\nabla_{\\theta} J(\\theta)$\n",
    "2. $s= \\beta_2 s + (1 - \\beta_2) \\nabla_{\\theta} \\theta J(\\theta) \\otimes \\nabla_{\\theta} \\theta J(\\theta)$\n",
    "3. $m = \\frac{m}{1-\\beta_1^T}$\n",
    "4. $s = \\frac{s}{1-\\beta_2^T}$\n",
    "5. $\\theta = \\theta - \\eta m \\oslash \\sqrt{s + \\epsilon}$\n",
    "\n",
    "Steps 1,2 and 5 are similar to Momentum optimization and RMSProp.\n",
    "\n",
    "Steps 3 and 4 are technical: since $m$ and $s$ are initialized at 0, they will be biased toward $0$ at the beginning of training, so these two steps will help boost $m$ and $s$ at the beginning of training.\n",
    "\n",
    "The momentum decay hyperparameter $\\beta_1$ is typically initialized to 0.9, while the scaling decay hyperparameter $\\beta_2$ is often initialized to 0.999. As earlier, the smoothing term $\\epsilon$ is usually initialized to a tiny number such as $10^–8$. These are the default values for TensorFlow’s AdamOptimizer class, so you can simply use:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a good learning rate can be tricky. If you set it way too high, training may actually diverge. If you set it too low, training will eventually converge to the optimum, but it will take a very long time.\n",
    "\n",
    "If you set it slightly too high, it will make progress very quickly at first, but it will end up dancing around the optimum, never settling down (unless you use an adaptive learning rate optimization algorithm such as AdaGrad, RMSProp, or Adam, but even then it may take time to settle).\n",
    "\n",
    "If you have a limited computing budget, you may have to interrupt training before it has converged properly, yielding a suboptimal solution.\n",
    "\n",
    "<p>\n",
    "    <img src=\"images\\Lesson10\\image4.png\" width=\"600\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try running the model several time for optimal learning rate. But you can do better:\n",
    "\n",
    "1. start with a high learning rate\n",
    "2. reduce it once it stops making fast progress\n",
    "\n",
    "These strategies are called learning schedules:\n",
    "1. Predetermined piecewise constant learning rate: set the learning rate to $\\eta_0 = 0.1$ at first, then to $\\eta_1 = 0.001$ after 50 epochs. Although this solution can work very well, it often requires fiddling around to figure out the right learning rates and when to use them.\n",
    "2. Exponential scheduling. Set the learning rate to a function of the iteration number t: $\\eta(t) = \\eta_0 + 10^{-t/r}$ This works great, but it requires tuning $\\eta_0$ and $r$. The learning rate will drop by a factor of 10 every $r$ steps.\n",
    "3. Power scheduling. Set the learning rate to $\\eta(t) = \\eta_0 (1 + t/r)^{-c}$. The hyperparameter $c$ is typically set to 1. This is similar to exponential scheduling, but the learning rate drops much more slowly.\n",
    "\n",
    "Testing showed that oth performance scheduling and exponential scheduling  performed the best, but power is easy and fast. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a generic model\n",
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Steps 10000, with decay rate. \n",
    "with tf.name_scope(\"train\"):       # not shown in the book\n",
    "    initial_learning_rate = 0.1\n",
    "    decay_steps = 10000\n",
    "    decay_rate = 1/10\n",
    "    global_step = tf.Variable(0, trainable=False, name=\"global_step\")\n",
    "    # prepackaged exponential decay\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate, global_step,\n",
    "                                               decay_steps, decay_rate)\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss, global_step=global_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.9574\n",
      "1 Validation accuracy: 0.9716\n",
      "2 Validation accuracy: 0.973\n",
      "3 Validation accuracy: 0.9798\n",
      "4 Validation accuracy: 0.9816\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 5\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avoiding Overfitting Through Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\ell_1$ and $\\ell_2$ regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement $\\ell_1$ regularization manually. First, we create the model, as usual (with just one hidden layer this time, for simplicity). Weights can be considered a regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    logits = tf.layers.dense(hidden1, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get a handle on the layer weights, and we compute the total loss, which is equal to the sum of the usual cross entropy loss and the $\\ell_1$ loss (i.e., the absolute values of the weights):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1 = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "W2 = tf.get_default_graph().get_tensor_by_name(\"outputs/kernel:0\")\n",
    "\n",
    "scale = 0.001 # l1 regularization hyperparameter\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                              logits=logits)\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")\n",
    "    # set up additional loss for both layers\n",
    "    reg_losses = tf.reduce_sum(tf.abs(W1)) + tf.reduce_sum(tf.abs(W2))\n",
    "    loss = tf.add(base_loss, scale * reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest is just as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Validation accuracy: 0.831\n",
      "1 Validation accuracy: 0.871\n",
      "2 Validation accuracy: 0.8838\n",
      "3 Validation accuracy: 0.8934\n",
      "4 Validation accuracy: 0.8966\n",
      "5 Validation accuracy: 0.8988\n",
      "6 Validation accuracy: 0.9016\n",
      "7 Validation accuracy: 0.9044\n",
      "8 Validation accuracy: 0.9058\n",
      "9 Validation accuracy: 0.906\n",
      "10 Validation accuracy: 0.9068\n",
      "11 Validation accuracy: 0.9054\n",
      "12 Validation accuracy: 0.907\n",
      "13 Validation accuracy: 0.9084\n",
      "14 Validation accuracy: 0.9088\n",
      "15 Validation accuracy: 0.9064\n",
      "16 Validation accuracy: 0.9068\n",
      "17 Validation accuracy: 0.9066\n",
      "18 Validation accuracy: 0.9066\n",
      "19 Validation accuracy: 0.9052\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can pass a regularization function to the `tf.layers.dense()` function, which will use it to create operations that will compute the regularization loss, and it adds these operations to the collection of regularization losses. The beginning is the same as above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will use Python's `partial()` function to avoid repeating the same arguments over and over again. Note that we set the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'partial' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-0b20dace37c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m my_dense_layer = partial(\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"dnn\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'partial' is not defined"
     ]
    }
   ],
   "source": [
    "my_dense_layer = partial(\n",
    "    tf.layers.dense, activation=tf.nn.relu,\n",
    "    kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = my_dense_layer(X, n_hidden1, name=\"hidden1\")\n",
    "    hidden2 = my_dense_layer(hidden1, n_hidden2, name=\"hidden2\")\n",
    "    logits = my_dense_layer(hidden2, n_outputs, activation=None,\n",
    "                            name=\"outputs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we must add the regularization losses to the base loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):                                     # not shown in the book\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(  # not shown\n",
    "        labels=y, logits=logits)                                # not shown\n",
    "    base_loss = tf.reduce_mean(xentropy, name=\"avg_xentropy\")   # not shown\n",
    "    reg_losses = tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss = tf.add_n([base_loss] + reg_losses, name=\"loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the rest is the same as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "learning_rate = 0.01\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 200\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: the book uses `tf.contrib.layers.dropout()` rather than `tf.layers.dropout()` (which did not exist when this chapter was written). It is now preferable to use `tf.layers.dropout()`, because anything in the contrib module may change or be deleted without notice. The `tf.layers.dropout()` function is almost identical to the `tf.contrib.layers.dropout()` function, except for a few minor differences. Most importantly:\n",
    "* you must specify the dropout rate (`rate`) rather than the keep probability (`keep_prob`), where `rate` is simply equal to `1 - keep_prob`,\n",
    "* the `is_training` parameter is renamed to `training`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most popular regularization technique for deep neural networks is arguably dropout. It was proposed by G. E. Hinton in 2012 and further detailed in a paper by Nitish Srivastava et al., and it has proven to be highly successful: even the state-of-the-art neural networks got a 1–2% accuracy boost simply by adding dropout. <br>\n",
    "\n",
    "At every training step, every neuron (including the input neurons but excluding the output neurons) has a probability $p$ of being temporarily “dropped out,” meaning it will be entirely ignored during this training step, but it may be active during the next step. The hyperparameter $p$ is called the dropout rate, and it is typically set to 50%. After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).\n",
    "\n",
    "<p>\n",
    "    <img src=\"images\\Lesson10\\image5.png\" width=\"600\" height=\"400\" />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suprisingly it works well. Dropping nodes makes networks less fragile and more resilient.\n",
    "\n",
    "\n",
    "Neurons trained with dropout cannot co-adapt with their neighboring neurons; they have to be as useful as possible on their own.\n",
    "They also cannot rely excessively on just a few input neurons; they must pay attention to each of their input neurons. They end up being less sensitive to slight changes in the inputs. \n",
    "\n",
    "\n",
    "\n",
    "Another way is to consider unique neural network is generated at each training step. Since each neuron can be either present or absent, there is a total of $2^N$ possible networks (where $N$ is the total number of droppable neurons). This is such a huge number that it is virtually impossible for the same neural network to be sampled twice. Once you have run a 10,000 training steps, you have essentially trained 10,000 different neural networks (each with just one training instance). These neural networks are obviously not independent since they share many of their weights, but they are nevertheless all different. The resulting neural network can be seen as an averaging ensemble of all these smaller neural networks.\n",
    "\n",
    "\n",
    "\n",
    "Suppose $p = 50$, in which case during testing a neuron will be connected to twice as many input neurons as it was (on average) during training. To compensate for this fact, we need to multiply each neuron’s input connection weights by 0.5 after training. If we don’t, each neuron will get a total input signal roughly twice as large as what the network was trained on, and it is unlikely to perform well. More generally, we need to multiply each input connection weight by the keep probability (1 – p) after training. Alternatively, we can divide each neuron’s output by the keep probability during training.\n",
    "\n",
    "To implement dropout using TensorFlow, you can simply apply the dropout() function to the input layer and to the output of every hidden layer. During training, this function randomly drops some items (setting them to 0) and divides the remaining items by the keep probability. After training, this function does nothing at all. The following code applies dropout regularization to our three-layer neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "# set dropout rate\n",
    "dropout_rate = 0.5  # == 1 - keep_prob\n",
    "X_drop = tf.layers.dropout(X, dropout_rate, training=training)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X_drop, n_hidden1, activation=tf.nn.relu,\n",
    "                              name=\"hidden1\")\n",
    "    # apply drop out rate\n",
    "    hidden1_drop = tf.layers.dropout(hidden1, dropout_rate, training=training)\n",
    "    hidden2 = tf.layers.dense(hidden1_drop, n_hidden2, activation=tf.nn.relu,\n",
    "                              name=\"hidden2\")\n",
    "    # apply drop out rate\n",
    "    hidden2_drop = tf.layers.dropout(hidden2, dropout_rate, training=training)\n",
    "    logits = tf.layers.dense(hidden2_drop, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum=0.9)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch, training: True})\n",
    "        accuracy_val = accuracy.eval(feed_dict={X: X_valid, y: y_valid})\n",
    "        print(epoch, \"Validation accuracy:\", accuracy_val)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you observe that the model is overfitting, you can increase the dropout rate. Conversely, you should try decreasing the dropout rate if the model underfits the training set.\n",
    "\n",
    "It can also help to increase the dropout rate for large layers, and reduce it for small ones. \n",
    "\n",
    "Dropout does tend to significantly slow down convergence, but it usually results in a much better model when tuned properly. So, it is generally well worth the extra time and effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cheat Sheet:\n",
    "\n",
    "Initialization : He\n",
    "\n",
    "Activation Function: ELE/SELU\n",
    "\n",
    "Normalization: Batch/None\n",
    "\n",
    "Regularization: Dropout\n",
    "\n",
    "Optimizer: Adam\n",
    "\n",
    "Learning Rate Schedule: None\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. If you can’t find a good learning rate (convergence was too slow, so you increased the training rate, and now convergence is fast but the network’s accuracy is suboptimal), then you can try adding a learning schedule such as exponential decay.\n",
    "\n",
    "2. If your training set is a bit too small, you can implement data augmentation.\n",
    "\n",
    "3. If you need a sparse model, you can add some ℓ1 regularization to the mix (and optionally zero out the tiny weights after training). \n",
    "4. If you need an even sparser model, you can try using FTRL instead of Adam optimization, along with ℓ1 regularization.\n",
    "5. If you need a lightning-fast model at runtime, you may want to drop Batch Normalization, and possibly replace the ELU activation function with the leaky ReLU. H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Max norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to a plain and simple neural net for MNIST with just 2 hidden layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")\n",
    "\n",
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's get a handle on the first hidden layer's weight and create an operation that will compute the clipped weights using the `clip_by_norm()` function. Then we create an assignment operation to assign the clipped weights to the weights variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "weights = tf.get_default_graph().get_tensor_by_name(\"hidden1/kernel:0\")\n",
    "clipped_weights = tf.clip_by_norm(weights, clip_norm=threshold, axes=1)\n",
    "clip_weights = tf.assign(weights, clipped_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do this as well for the second hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights2 = tf.get_default_graph().get_tensor_by_name(\"hidden2/kernel:0\")\n",
    "clipped_weights2 = tf.clip_by_norm(weights2, clip_norm=threshold, axes=1)\n",
    "clip_weights2 = tf.assign(weights2, clipped_weights2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add an initializer and a saver:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can train the model. It's pretty much as usual, except that right after running the `training_op`, we run the `clip_weights` and `clip_weights2` operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:                                              # not shown in the book\n",
    "    init.run()                                                          # not shown\n",
    "    for epoch in range(n_epochs):                                       # not shown\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size): # not shown\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            clip_weights.eval()\n",
    "            clip_weights2.eval()                                        # not shown\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid})   # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)                 # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")               # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The implementation above is straightforward and it works fine, but it is a bit messy. A better approach is to define a `max_norm_regularizer()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_norm_regularizer(threshold, axes=1, name=\"max_norm\",\n",
    "                         collection=\"max_norm\"):\n",
    "    def max_norm(weights):\n",
    "        clipped = tf.clip_by_norm(weights, clip_norm=threshold, axes=axes)\n",
    "        clip_weights = tf.assign(weights, clipped, name=name)\n",
    "        tf.add_to_collection(collection, clip_weights)\n",
    "        return None # there is no regularization loss term\n",
    "    return max_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you can call this function to get a max norm regularizer (with the threshold you want). When you create a hidden layer, you can pass this regularizer to the `kernel_regularizer` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 50\n",
    "n_outputs = 10\n",
    "\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_norm_reg = max_norm_regularizer(threshold=1.0)\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    hidden1 = tf.layers.dense(X, n_hidden1, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden1\")\n",
    "    hidden2 = tf.layers.dense(hidden1, n_hidden2, activation=tf.nn.relu,\n",
    "                              kernel_regularizer=max_norm_reg, name=\"hidden2\")\n",
    "    logits = tf.layers.dense(hidden2, n_outputs, name=\"outputs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.name_scope(\"loss\"):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "    loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "with tf.name_scope(\"train\"):\n",
    "    optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "    training_op = optimizer.minimize(loss)    \n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logits, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is as usual, except you must run the weights clipping operations after each training operation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 20\n",
    "batch_size = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clip_all_weights = tf.get_collection(\"max_norm\")\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for X_batch, y_batch in shuffle_batch(X_train, y_train, batch_size):\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "            sess.run(clip_all_weights)\n",
    "        acc_valid = accuracy.eval(feed_dict={X: X_valid, y: y_valid}) # not shown\n",
    "        print(epoch, \"Validation accuracy:\", acc_valid)               # not shown\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_model_final.ckpt\")             # not shown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Exercise solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. to 7."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See appendix A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Build a DNN with five hidden layers of 100 neurons each, He initialization, and the ELU activation function._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will need similar DNNs in the next exercises, so let's create a function to build this DNN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "he_init = tf.variance_scaling_initializer()\n",
    "\n",
    "def dnn(inputs, n_hidden_layers=5, n_neurons=100, name=None,\n",
    "        activation=tf.nn.elu, initializer=he_init):\n",
    "    with tf.variable_scope(name, \"dnn\"):\n",
    "        for layer in range(n_hidden_layers):\n",
    "            inputs = tf.layers.dense(inputs, n_neurons, activation=activation,\n",
    "                                     kernel_initializer=initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "        return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "n_outputs = 5\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Using Adam optimization and early stopping, try training it on MNIST but only on digits 0 to 4, as we will use transfer learning for digits 5 to 9 in the next exercise. You will need a softmax output layer with five neurons, and as always make sure to save checkpoints at regular intervals and save the final model so you can reuse it later._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete the graph with the cost function, the training op, and all the other usual components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "training_op = optimizer.minimize(loss, name=\"training_op\")\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the training set, validation and test set (we need the validation set to implement early stopping):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train[y_train < 5]\n",
    "y_train1 = y_train[y_train < 5]\n",
    "X_valid1 = X_valid[y_valid < 5]\n",
    "y_valid1 = y_valid[y_valid < 5]\n",
    "X_test1 = X_test[y_test < 5]\n",
    "y_test1 = y_test[y_test < 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train1))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = X_train1[rnd_indices], y_train1[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid1, y: y_valid1})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = saver.save(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, \"./my_mnist_model_0_to_4.ckpt\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get 98.05% accuracy on the test set. That's not too bad, but let's see if we can do better by tuning the hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Tune the hyperparameters using cross-validation and see what precision you can achieve._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a `DNNClassifier` class, compatible with Scikit-Learn's `RandomizedSearchCV` class, to perform hyperparameter tuning. Here are the key points of this implementation:\n",
    "* the `__init__()` method (constructor) does nothing more than create instance variables for each of the hyperparameters.\n",
    "* the `fit()` method creates the graph, starts a session and trains the model:\n",
    "  * it calls the `_build_graph()` method to build the graph (much lile the graph we defined earlier). Once this method is done creating the graph, it saves all the important operations as instance variables for easy access by other methods.\n",
    "  * the `_dnn()` method builds the hidden layers, just like the `dnn()` function above, but also with support for batch normalization and dropout (for the next exercises).\n",
    "  * if the `fit()` method is given a validation set (`X_valid` and `y_valid`), then it implements early stopping. This implementation does not save the best model to disk, but rather to memory: it uses the `_get_model_params()` method to get all the graph's variables and their values, and the `_restore_model_params()` method to restore the variable values (of the best model found). This trick helps speed up training.\n",
    "  * After the `fit()` method has finished training the model, it keeps the session open so that predictions can be made quickly, without having to save a model to disk and restore it for every prediction. You can close the session by calling the `close_session()` method.\n",
    "* the `predict_proba()` method uses the trained model to predict the class probabilities.\n",
    "* the `predict()` method calls `predict_proba()` and returns the class with the highest probability, for each instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.exceptions import NotFittedError\n",
    "\n",
    "class DNNClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, n_hidden_layers=5, n_neurons=100, optimizer_class=tf.train.AdamOptimizer,\n",
    "                 learning_rate=0.01, batch_size=20, activation=tf.nn.elu, initializer=he_init,\n",
    "                 batch_norm_momentum=None, dropout_rate=None, random_state=None):\n",
    "        \"\"\"Initialize the DNNClassifier by simply storing all the hyperparameters.\"\"\"\n",
    "        self.n_hidden_layers = n_hidden_layers\n",
    "        self.n_neurons = n_neurons\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.learning_rate = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.activation = activation\n",
    "        self.initializer = initializer\n",
    "        self.batch_norm_momentum = batch_norm_momentum\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.random_state = random_state\n",
    "        self._session = None\n",
    "\n",
    "    def _dnn(self, inputs):\n",
    "        \"\"\"Build the hidden layers, with support for batch normalization and dropout.\"\"\"\n",
    "        for layer in range(self.n_hidden_layers):\n",
    "            if self.dropout_rate:\n",
    "                inputs = tf.layers.dropout(inputs, self.dropout_rate, training=self._training)\n",
    "            inputs = tf.layers.dense(inputs, self.n_neurons,\n",
    "                                     kernel_initializer=self.initializer,\n",
    "                                     name=\"hidden%d\" % (layer + 1))\n",
    "            if self.batch_norm_momentum:\n",
    "                inputs = tf.layers.batch_normalization(inputs, momentum=self.batch_norm_momentum,\n",
    "                                                       training=self._training)\n",
    "            inputs = self.activation(inputs, name=\"hidden%d_out\" % (layer + 1))\n",
    "        return inputs\n",
    "\n",
    "    def _build_graph(self, n_inputs, n_outputs):\n",
    "        \"\"\"Build the same model as earlier\"\"\"\n",
    "        if self.random_state is not None:\n",
    "            tf.set_random_seed(self.random_state)\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "        y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "        if self.batch_norm_momentum or self.dropout_rate:\n",
    "            self._training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "        else:\n",
    "            self._training = None\n",
    "\n",
    "        dnn_outputs = self._dnn(X)\n",
    "\n",
    "        logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init, name=\"logits\")\n",
    "        Y_proba = tf.nn.softmax(logits, name=\"Y_proba\")\n",
    "\n",
    "        xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n",
    "                                                                  logits=logits)\n",
    "        loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "        optimizer = self.optimizer_class(learning_rate=self.learning_rate)\n",
    "        training_op = optimizer.minimize(loss)\n",
    "\n",
    "        correct = tf.nn.in_top_k(logits, y, 1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "        init = tf.global_variables_initializer()\n",
    "        saver = tf.train.Saver()\n",
    "\n",
    "        # Make the important operations available easily through instance variables\n",
    "        self._X, self._y = X, y\n",
    "        self._Y_proba, self._loss = Y_proba, loss\n",
    "        self._training_op, self._accuracy = training_op, accuracy\n",
    "        self._init, self._saver = init, saver\n",
    "\n",
    "    def close_session(self):\n",
    "        if self._session:\n",
    "            self._session.close()\n",
    "\n",
    "    def _get_model_params(self):\n",
    "        \"\"\"Get all variable values (used for early stopping, faster than saving to disk)\"\"\"\n",
    "        with self._graph.as_default():\n",
    "            gvars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES)\n",
    "        return {gvar.op.name: value for gvar, value in zip(gvars, self._session.run(gvars))}\n",
    "\n",
    "    def _restore_model_params(self, model_params):\n",
    "        \"\"\"Set all variables to the given values (for early stopping, faster than loading from disk)\"\"\"\n",
    "        gvar_names = list(model_params.keys())\n",
    "        assign_ops = {gvar_name: self._graph.get_operation_by_name(gvar_name + \"/Assign\")\n",
    "                      for gvar_name in gvar_names}\n",
    "        init_values = {gvar_name: assign_op.inputs[1] for gvar_name, assign_op in assign_ops.items()}\n",
    "        feed_dict = {init_values[gvar_name]: model_params[gvar_name] for gvar_name in gvar_names}\n",
    "        self._session.run(assign_ops, feed_dict=feed_dict)\n",
    "\n",
    "    def fit(self, X, y, n_epochs=100, X_valid=None, y_valid=None):\n",
    "        \"\"\"Fit the model to the training set. If X_valid and y_valid are provided, use early stopping.\"\"\"\n",
    "        self.close_session()\n",
    "\n",
    "        # infer n_inputs and n_outputs from the training set.\n",
    "        n_inputs = X.shape[1]\n",
    "        self.classes_ = np.unique(y)\n",
    "        n_outputs = len(self.classes_)\n",
    "        \n",
    "        # Translate the labels vector to a vector of sorted class indices, containing\n",
    "        # integers from 0 to n_outputs - 1.\n",
    "        # For example, if y is equal to [8, 8, 9, 5, 7, 6, 6, 6], then the sorted class\n",
    "        # labels (self.classes_) will be equal to [5, 6, 7, 8, 9], and the labels vector\n",
    "        # will be translated to [3, 3, 4, 0, 2, 1, 1, 1]\n",
    "        self.class_to_index_ = {label: index\n",
    "                                for index, label in enumerate(self.classes_)}\n",
    "        y = np.array([self.class_to_index_[label]\n",
    "                      for label in y], dtype=np.int32)\n",
    "        \n",
    "        self._graph = tf.Graph()\n",
    "        with self._graph.as_default():\n",
    "            self._build_graph(n_inputs, n_outputs)\n",
    "            # extra ops for batch normalization\n",
    "            extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "        # needed in case of early stopping\n",
    "        max_checks_without_progress = 20\n",
    "        checks_without_progress = 0\n",
    "        best_loss = np.infty\n",
    "        best_params = None\n",
    "        \n",
    "        # Now train the model!\n",
    "        self._session = tf.Session(graph=self._graph)\n",
    "        with self._session.as_default() as sess:\n",
    "            self._init.run()\n",
    "            for epoch in range(n_epochs):\n",
    "                rnd_idx = np.random.permutation(len(X))\n",
    "                for rnd_indices in np.array_split(rnd_idx, len(X) // self.batch_size):\n",
    "                    X_batch, y_batch = X[rnd_indices], y[rnd_indices]\n",
    "                    feed_dict = {self._X: X_batch, self._y: y_batch}\n",
    "                    if self._training is not None:\n",
    "                        feed_dict[self._training] = True\n",
    "                    sess.run(self._training_op, feed_dict=feed_dict)\n",
    "                    if extra_update_ops:\n",
    "                        sess.run(extra_update_ops, feed_dict=feed_dict)\n",
    "                if X_valid is not None and y_valid is not None:\n",
    "                    loss_val, acc_val = sess.run([self._loss, self._accuracy],\n",
    "                                                 feed_dict={self._X: X_valid,\n",
    "                                                            self._y: y_valid})\n",
    "                    if loss_val < best_loss:\n",
    "                        best_params = self._get_model_params()\n",
    "                        best_loss = loss_val\n",
    "                        checks_without_progress = 0\n",
    "                    else:\n",
    "                        checks_without_progress += 1\n",
    "                    print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_val, best_loss, acc_val * 100))\n",
    "                    if checks_without_progress > max_checks_without_progress:\n",
    "                        print(\"Early stopping!\")\n",
    "                        break\n",
    "                else:\n",
    "                    loss_train, acc_train = sess.run([self._loss, self._accuracy],\n",
    "                                                     feed_dict={self._X: X_batch,\n",
    "                                                                self._y: y_batch})\n",
    "                    print(\"{}\\tLast training batch loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "                        epoch, loss_train, acc_train * 100))\n",
    "            # If we used early stopping then rollback to the best model found\n",
    "            if best_params:\n",
    "                self._restore_model_params(best_params)\n",
    "            return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        if not self._session:\n",
    "            raise NotFittedError(\"This %s instance is not fitted yet\" % self.__class__.__name__)\n",
    "        with self._session.as_default() as sess:\n",
    "            return self._Y_proba.eval(feed_dict={self._X: X})\n",
    "\n",
    "    def predict(self, X):\n",
    "        class_indices = np.argmax(self.predict_proba(X), axis=1)\n",
    "        return np.array([[self.classes_[class_index]]\n",
    "                         for class_index in class_indices], np.int32)\n",
    "\n",
    "    def save(self, path):\n",
    "        self._saver.save(self._session, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we get the exact same accuracy as earlier using this class (without dropout or batch norm):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model is trained, let's see if it gets the same accuracy as earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yep! Working fine. Now we can use Scikit-Learn's `RandomizedSearchCV` class to search for better hyperparameters (this may take over an hour, depending on your system):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "def leaky_relu(alpha=0.01):\n",
    "    def parametrized_leaky_relu(z, name=None):\n",
    "        return tf.maximum(alpha * z, z, name=name)\n",
    "    return parametrized_leaky_relu\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "}\n",
    "\n",
    "rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                random_state=42, verbose=2)\n",
    "rnd_search.fit(X_train1, y_train1, X_valid=X_valid1, y_valid=y_valid1, n_epochs=1000)\n",
    "\n",
    "# If you have Scikit-Learn 0.18 or earlier, you should upgrade, or use the fit_params argument:\n",
    "# fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000}\n",
    "# rnd_search = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "#                                 fit_params=fit_params, random_state=42, verbose=2)\n",
    "# rnd_search.fit(X_train1, y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wonderful! Tuning the hyperparameters got us up to 99.32% accuracy! It may not sound like a great improvement to go from 98.05% to 99.32% accuracy, but consider the error rate: it went from roughly 2% to 0.7%. That's a 65% reduction of the number of errors this model will produce!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a good idea to save this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search.best_estimator_.save(\"./my_best_mnist_model_0_to_4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: Now try adding Batch Normalization and compare the learning curves: is it converging faster than before? Does it produce a better model?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train the best model found, once again, to see how fast it converges (alternatively, you could tweak the code above to make it write summaries for TensorBoard, so you can visualize the learning curve):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                        n_neurons=140, random_state=42)\n",
    "dnn_clf.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best loss is reached at epoch 19, but it was already within 10% of that result at epoch 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check that we do indeed get 99.32% accuracy on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good, now let's use the exact same model, but this time with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_bn = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                           n_neurons=90, random_state=42,\n",
    "                           batch_norm_momentum=0.95)\n",
    "dnn_clf_bn.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best params are reached during epoch 48, that's actually a slower convergence than earlier. Let's check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, batch normalization did not improve accuracy. Let's see if we can find a good set of hyperparameters that will work well with batch normalization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"batch_norm_momentum\": [0.9, 0.95, 0.98, 0.99, 0.999],\n",
    "}\n",
    "\n",
    "rnd_search_bn = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                   fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                   random_state=42, verbose=2)\n",
    "rnd_search_bn.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_bn.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_bn.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Slightly better than earlier: 99.4% vs 99.3%. Let's see if dropout can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: is the model overfitting the training set? Try adding dropout to every layer and try again. Does it help?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's go back to the best model we trained earlier and see how it performs on the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf.predict(X_train1)\n",
    "accuracy_score(y_train1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs significantly better on the training set than on the test set (99.91% vs 99.32%), which means it is overfitting the training set. A bit of regularization may help. Let's try adding dropout with a 50% dropout rate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_dropout = DNNClassifier(activation=leaky_relu(alpha=0.1), batch_size=500, learning_rate=0.01,\n",
    "                                n_neurons=90, random_state=42,\n",
    "                                dropout_rate=0.5)\n",
    "dnn_clf_dropout.fit(X_train1, y_train1, n_epochs=1000, X_valid=X_valid1, y_valid=y_valid1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best params are reached during epoch 23. Dropout somewhat slowed down convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are out of luck, dropout does not seem to help either. Let's try tuning the hyperparameters, perhaps we can squeeze a bit more performance out of this model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_distribs = {\n",
    "    \"n_neurons\": [10, 30, 50, 70, 90, 100, 120, 140, 160],\n",
    "    \"batch_size\": [10, 50, 100, 500],\n",
    "    \"learning_rate\": [0.01, 0.02, 0.05, 0.1],\n",
    "    \"activation\": [tf.nn.relu, tf.nn.elu, leaky_relu(alpha=0.01), leaky_relu(alpha=0.1)],\n",
    "    # you could also try exploring different numbers of hidden layers, different optimizers, etc.\n",
    "    #\"n_hidden_layers\": [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
    "    #\"optimizer_class\": [tf.train.AdamOptimizer, partial(tf.train.MomentumOptimizer, momentum=0.95)],\n",
    "    \"dropout_rate\": [0.2, 0.3, 0.4, 0.5, 0.6],\n",
    "}\n",
    "\n",
    "rnd_search_dropout = RandomizedSearchCV(DNNClassifier(random_state=42), param_distribs, n_iter=50,\n",
    "                                        fit_params={\"X_valid\": X_valid1, \"y_valid\": y_valid1, \"n_epochs\": 1000},\n",
    "                                        random_state=42, verbose=2)\n",
    "rnd_search_dropout.fit(X_train1, y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnd_search_dropout.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rnd_search_dropout.predict(X_test1)\n",
    "accuracy_score(y_test1, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh well, dropout did not improve the model. Better luck next time! :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But that's okay, we have ourselves a nice DNN that achieves 99.40% accuracy on the test set using Batch Normalization, or 99.32% without BN. Let's see if some of this expertise on digits 0 to 4 can be transferred to the task of classifying digits 5 to 9. For the sake of simplicity we will reuse the DNN without BN, since it is almost as good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: create a new DNN that reuses all the pretrained hidden layers of the previous model, freezes them, and replaces the softmax output layer with a new one._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the best model's graph and get a handle on all the important operations we will need. Note that instead of creating a new softmax output layer, we will just reuse the existing one (since it has the same number of outputs as the existing one). We will reinitialize its parameters before training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "loss = tf.get_default_graph().get_tensor_by_name(\"loss:0\")\n",
    "Y_proba = tf.get_default_graph().get_tensor_by_name(\"Y_proba:0\")\n",
    "logits = Y_proba.op.inputs[0]\n",
    "accuracy = tf.get_default_graph().get_tensor_by_name(\"accuracy:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To freeze the lower layers, we will exclude their variables from the optimizer's list of trainable variables, keeping only the output layer's trainable variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "five_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: train this new DNN on digits 5 to 9, using only 100 images per digit, and time how long it takes. Despite this small number of examples, can you achieve high precision?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the training, validation and test sets. We need to subtract 5 from the labels because TensorFlow expects integers from 0 to `n_classes-1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2_full = X_train[y_train >= 5]\n",
    "y_train2_full = y_train[y_train >= 5] - 5\n",
    "X_valid2_full = X_valid[y_valid >= 5]\n",
    "y_valid2_full = y_valid[y_valid >= 5] - 5\n",
    "X_test2 = X_test[y_test >= 5]\n",
    "y_test2 = y_test[y_test >= 5] - 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, for the purpose of this exercise, we want to keep only 100 instances per class in the training set (and let's keep only 30 instances per class in the validation set). Let's create a small function to do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_n_instances_per_class(X, y, n=100):\n",
    "    Xs, ys = [], []\n",
    "    for label in np.unique(y):\n",
    "        idx = (y == label)\n",
    "        Xc = X[idx][:n]\n",
    "        yc = y[idx][:n]\n",
    "        Xs.append(Xc)\n",
    "        ys.append(yc)\n",
    "    return np.concatenate(Xs), np.concatenate(ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train2, y_train2 = sample_n_instances_per_class(X_train2_full, y_train2_full, n=100)\n",
    "X_valid2, y_valid2 = sample_n_instances_per_class(X_valid2_full, y_valid2_full, n=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model. This is the same training code as earlier, using early stopping, except for the initialization: we first initialize all the variables, then we restore the best model trained earlier (on digits 0 to 4), and finally we reinitialize the output layer variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well that's not a great accuracy, is it? Of course with such a tiny training set, and with only one layer to tweak, we should not expect miracles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: try caching the frozen layers, and train the model again: how much faster is it now?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting a handle on the output of the last frozen layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden5_out = tf.get_default_graph().get_tensor_by_name(\"hidden5_out:0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model using roughly the same code as earlier. The difference is that we compute the output of the top frozen layer at the beginning (both for the training set and the validation set), and we cache it. This makes training roughly 1.5 to 3 times faster in this example (this may vary greatly, depending on your system): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "    for var in output_layer_vars:\n",
    "        var.initializer.run()\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    hidden5_train = hidden5_out.eval(feed_dict={X: X_train2, y: y_train2})\n",
    "    hidden5_valid = hidden5_out.eval(feed_dict={X: X_valid2, y: y_valid2})\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            h5_batch, y_batch = hidden5_train[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={hidden5_out: h5_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={hidden5_out: hidden5_valid, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = five_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "    t1 = time.time()\n",
    "    print(\"Total training time: {:.1f}s\".format(t1 - t0))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    five_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_five_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: try again reusing just four hidden layers instead of five. Can you achieve a higher precision?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load the best model again, but this time we will create a new softmax output layer on top of the 4th hidden layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_outputs = 5\n",
    "\n",
    "restore_saver = tf.train.import_meta_graph(\"./my_best_mnist_model_0_to_4.meta\")\n",
    "\n",
    "X = tf.get_default_graph().get_tensor_by_name(\"X:0\")\n",
    "y = tf.get_default_graph().get_tensor_by_name(\"y:0\")\n",
    "\n",
    "hidden4_out = tf.get_default_graph().get_tensor_by_name(\"hidden4_out:0\")\n",
    "logits = tf.layers.dense(hidden4_out, n_outputs, kernel_initializer=he_init, name=\"new_logits\")\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32), name=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's create the training operation. We want to freeze all the layers except for the new output layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "output_layer_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam2\")\n",
    "training_op = optimizer.minimize(loss, var_list=output_layer_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "four_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And once again we train the model with the same code as earlier. Note: we could of course write a function once and use it multiple times, rather than copying almost the same training code over and over again, but as we keep tweaking the code slightly, the function would need multiple arguments and `if` statements, and it would have to be at the beginning of the notebook, where it would not make much sense to readers. In short it would be very confusing, so we're better off with copy & paste."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_best_mnist_model_0_to_4\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = four_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still not fantastic, but much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Exercise: now unfreeze the top two hidden layers and continue training: can you get the model to perform even better?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "unfrozen_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=\"hidden[34]|new_logits\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam3\")\n",
    "training_op = optimizer.minimize(loss, var_list=unfrozen_vars)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "two_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    four_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_four_frozen\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = two_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check what accuracy we can get by unfreezing all layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate, name=\"Adam4\")\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "no_frozen_saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 1000\n",
    "batch_size = 20\n",
    "\n",
    "max_checks_without_progress = 20\n",
    "checks_without_progress = 0\n",
    "best_loss = np.infty\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    two_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_two_frozen\")\n",
    "        \n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        loss_val, acc_val = sess.run([loss, accuracy], feed_dict={X: X_valid2, y: y_valid2})\n",
    "        if loss_val < best_loss:\n",
    "            save_path = no_frozen_saver.save(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n",
    "            best_loss = loss_val\n",
    "            checks_without_progress = 0\n",
    "        else:\n",
    "            checks_without_progress += 1\n",
    "            if checks_without_progress > max_checks_without_progress:\n",
    "                print(\"Early stopping!\")\n",
    "                break\n",
    "        print(\"{}\\tValidation loss: {:.6f}\\tBest loss: {:.6f}\\tAccuracy: {:.2f}%\".format(\n",
    "            epoch, loss_val, best_loss, acc_val * 100))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    no_frozen_saver.restore(sess, \"./my_mnist_model_5_to_9_no_frozen\")\n",
    "    acc_test = accuracy.eval(feed_dict={X: X_test2, y: y_test2})\n",
    "    print(\"Final test accuracy: {:.2f}%\".format(acc_test * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare that to a DNN trained from scratch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_clf_5_to_9 = DNNClassifier(n_hidden_layers=4, random_state=42)\n",
    "dnn_clf_5_to_9.fit(X_train2, y_train2, n_epochs=1000, X_valid=X_valid2, y_valid=y_valid2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = dnn_clf_5_to_9.predict(X_test2)\n",
    "accuracy_score(y_test2, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meh. How disappointing! ;) Transfer learning did not help much (if at all) in this task. At least we tried... Fortunately, the next exercise will get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Pretraining on an auxiliary task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise you will build a DNN that compares two MNIST digit images and predicts whether they represent the same digit or not. Then you will reuse the lower layers of this network to train an MNIST classifier using very little training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.1.\n",
    "Exercise: _Start by building two DNNs (let's call them DNN A and B), both similar to the one you built earlier but without the output layer: each DNN should have five hidden layers of 100 neurons each, He initialization, and ELU activation. Next, add one more hidden layer with 10 units on top of both DNNs. You should use TensorFlow's `concat()` function with `axis=1` to concatenate the outputs of both DNNs along the horizontal axis, then feed the result to the hidden layer. Finally, add an output layer with a single neuron using the logistic activation function._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Warning**! There was an error in the book for this exercise: there was no instruction to add a top hidden layer. Without it, the neural network generally fails to start learning. If you have the latest version of the book, this error has been fixed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You could have two input placeholders, `X1` and `X2`, one for the images that should be fed to the first DNN, and the other for the images that should be fed to the second DNN. It would work fine. However, another option is to have a single input placeholder to hold both sets of images (each row will hold a pair of images), and use `tf.unstack()` to split this tensor into two separate tensors, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 28 * 28 # MNIST\n",
    "\n",
    "reset_graph()\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, 2, n_inputs), name=\"X\")\n",
    "X1, X2 = tf.unstack(X, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need the labels placeholder. Each label will be 0 if the images represent different digits, or 1 if they represent the same digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.placeholder(tf.int32, shape=[None, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's feed these inputs through two separate DNNs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn1 = dnn(X1, name=\"DNN_A\")\n",
    "dnn2 = dnn(X2, name=\"DNN_B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's concatenate their outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_outputs = tf.concat([dnn1, dnn2], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each DNN outputs 100 activations (per instance), so the shape is `[None, 100]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn2.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And of course the concatenated outputs have a shape of `[None, 200]`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dnn_outputs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets add an extra hidden layer with just 10 neurons, and the output layer, with a single neuron:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden = tf.layers.dense(dnn_outputs, units=10, activation=tf.nn.elu, kernel_initializer=he_init)\n",
    "logits = tf.layers.dense(hidden, units=1, kernel_initializer=he_init)\n",
    "y_proba = tf.nn.sigmoid(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The whole network predicts `1` if `y_proba >= 0.5` (i.e. the network predicts that the images represent the same digit), or `0` otherwise. We compute instead `logits >= 0`, which is equivalent but faster to compute: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.cast(tf.greater_equal(logits, 0), tf.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's add the cost function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_as_float = tf.cast(y, tf.float32)\n",
    "xentropy = tf.nn.sigmoid_cross_entropy_with_logits(labels=y_as_float, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can now create the training operation using an optimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "momentum = 0.95\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will want to measure our classifier's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_correct = tf.equal(y_pred, y)\n",
    "accuracy = tf.reduce_mean(tf.cast(y_pred_correct, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the usual `init` and `saver`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.2.\n",
    "_Exercise: split the MNIST training set in two sets: split #1 should containing 55,000 images, and split #2 should contain contain 5,000 images. Create a function that generates a training batch where each instance is a pair of MNIST images picked from split #1. Half of the training instances should be pairs of images that belong to the same class, while the other half should be images from different classes. For each pair, the training label should be 0 if the images are from the same class, or 1 if they are from different classes._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The MNIST dataset returned by TensorFlow's `input_data()` function is already split into 3 parts: a training set (55,000 instances), a validation set (5,000 instances) and a test set (10,000 instances). Let's use the first set to generate the training set composed image pairs, and we will use the second set for the second phase of the exercise (to train a regular MNIST classifier). We will use the third set as the test set for both phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = X_train\n",
    "y_train1 = y_train\n",
    "\n",
    "X_train2 = X_valid\n",
    "y_train2 = y_valid\n",
    "\n",
    "X_test = X_test\n",
    "y_test = y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that generates pairs of images: 50% representing the same digit, and 50% representing different digits. There are many ways to implement this. In this implementation, we first decide how many \"same\" pairs (i.e. pairs of images representing the same digit) we will generate, and how many \"different\" pairs (i.e. pairs of images representing different digits). We could just use `batch_size // 2` but we want to handle the case where it is odd (granted, that might be overkill!). Then we generate random pairs and we pick the right number of \"same\" pairs, then we generate the right number of \"different\" pairs. Finally we shuffle the batch and return it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(images, labels, batch_size):\n",
    "    size1 = batch_size // 2\n",
    "    size2 = batch_size - size1\n",
    "    if size1 != size2 and np.random.rand() > 0.5:\n",
    "        size1, size2 = size2, size1\n",
    "    X = []\n",
    "    y = []\n",
    "    while len(X) < size1:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if rnd_idx1 != rnd_idx2 and labels[rnd_idx1] == labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([1])\n",
    "    while len(X) < batch_size:\n",
    "        rnd_idx1, rnd_idx2 = np.random.randint(0, len(images), 2)\n",
    "        if labels[rnd_idx1] != labels[rnd_idx2]:\n",
    "            X.append(np.array([images[rnd_idx1], images[rnd_idx2]]))\n",
    "            y.append([0])\n",
    "    rnd_indices = np.random.permutation(batch_size)\n",
    "    return np.array(X)[rnd_indices], np.array(y)[rnd_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it to generate a small batch of 5 image pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 5\n",
    "X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in `X_batch` contains a pair of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape, X_batch.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at these pairs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3, 3 * batch_size))\n",
    "plt.subplot(121)\n",
    "plt.imshow(X_batch[:,0].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.subplot(122)\n",
    "plt.imshow(X_batch[:,1].reshape(28 * batch_size, 28), cmap=\"binary\", interpolation=\"nearest\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's look at the labels (0 means \"different\", 1 means \"same\"):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.3.\n",
    "_Exercise: train the DNN on this training set. For each image pair, you can simultaneously feed the first image to DNN A and the second image to DNN B. The whole network will gradually learn to tell whether two images belong to the same class or not._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate a test set composed of many pairs of images pulled from the MNIST test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test1, y_test1 = generate_batch(X_test, y_test, batch_size=len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now, let's train the model. There's really nothing special about this step, except for the fact that we need a fairly large `batch_size`, otherwise the model fails to learn anything and ends up with an accuracy of 50%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 500\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epochs):\n",
    "        for iteration in range(len(X_train1) // batch_size):\n",
    "            X_batch, y_batch = generate_batch(X_train1, y_train1, batch_size)\n",
    "            loss_val, _ = sess.run([loss, training_op], feed_dict={X: X_batch, y: y_batch})\n",
    "        print(epoch, \"Train loss:\", loss_val)\n",
    "        if epoch % 5 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test1, y: y_test1})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_digit_comparison_model.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All right, we reach 97.6% accuracy on this digit comparison task. That's not too bad, this model knows a thing or two about comparing handwritten digits!\n",
    "\n",
    "Let's see if some of that knowledge can be useful for the regular MNIST classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10.4.\n",
    "_Exercise: now create a new DNN by reusing and freezing the hidden layers of DNN A and adding a softmax output layer on top with 10 neurons. Train this network on split #2 and see if you can achieve high performance despite having only 500 images per class._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create the model, it is pretty straightforward. There are many ways to freeze the lower layers, as explained in the book. In this example, we chose to use the `tf.stop_gradient()` function. Note that we need one `Saver` to restore the pretrained DNN A, and another `Saver` to save the final model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "frozen_outputs = tf.stop_gradient(dnn_outputs)\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now on to training! We first initialize all variables (including the variables in the new output layer), then we restore the pretrained DNN A. Next, we just train the model on the small MNIST dataset (containing just 5,000 images):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 100\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    restore_saver.restore(sess, \"./my_digit_comparison_model.ckpt\")\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, 96.7% accuracy, that's not the best MNIST model we have trained so far, but recall that we are only using a small training set (just 500 images per digit). Let's compare this result with the same DNN trained from scratch, without using transfer learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset_graph()\n",
    "\n",
    "n_inputs = 28 * 28  # MNIST\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int32, shape=(None), name=\"y\")\n",
    "\n",
    "dnn_outputs = dnn(X, name=\"DNN_A\")\n",
    "\n",
    "logits = tf.layers.dense(dnn_outputs, n_outputs, kernel_initializer=he_init)\n",
    "Y_proba = tf.nn.softmax(logits)\n",
    "\n",
    "xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=logits)\n",
    "loss = tf.reduce_mean(xentropy, name=\"loss\")\n",
    "\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate, momentum, use_nesterov=True)\n",
    "training_op = optimizer.minimize(loss)\n",
    "\n",
    "correct = tf.nn.in_top_k(logits, y, 1)\n",
    "accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "dnn_A_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope=\"DNN_A\")\n",
    "restore_saver = tf.train.Saver(var_list={var.op.name: var for var in dnn_A_vars})\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 150\n",
    "batch_size = 50\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        rnd_idx = np.random.permutation(len(X_train2))\n",
    "        for rnd_indices in np.array_split(rnd_idx, len(X_train2) // batch_size):\n",
    "            X_batch, y_batch = X_train2[rnd_indices], y_train2[rnd_indices]\n",
    "            sess.run(training_op, feed_dict={X: X_batch, y: y_batch})\n",
    "        if epoch % 10 == 0:\n",
    "            acc_test = accuracy.eval(feed_dict={X: X_test, y: y_test})\n",
    "            print(epoch, \"Test accuracy:\", acc_test)\n",
    "\n",
    "    save_path = saver.save(sess, \"./my_mnist_model_final.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only 94.8% accuracy... So transfer learning helped us reduce the error rate from 5.2% to 3.3% (that's over 36% error reduction). Moreover, the model using transfer learning reached over 96% accuracy in less than 10 epochs.\n",
    "\n",
    "Bottom line: transfer learning does not always work (as we saw in exercise 9), but when it does it can make a big difference. So try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "nav_menu": {
   "height": "360px",
   "width": "416px"
  },
  "toc": {
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 6,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
